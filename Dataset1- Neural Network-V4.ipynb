{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39ad1a2a",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff62b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic packages\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#data pre-processing packages\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#results and analysis packages\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78cf529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data modelling & results\n",
    "from yellowbrick.regressor import PredictionError, ResidualsPlot\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#NN\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#feature importance\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5304bd4d",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd727e",
   "metadata": {},
   "source": [
    "## Error computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a4247b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the Root Mean Squared Error\n",
    "\n",
    "def rmse(y_true, y_predicted):\n",
    "    \n",
    "    return np.sqrt(mean_squared_error(y_true, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "910f4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#errors computation\n",
    "\n",
    "def errors_computation(data):\n",
    "    \n",
    "    df=pd.DataFrame()\n",
    "    #df.at['RMSE (as root mean)', 'Wind']= round(rmse(data['Target'], data['WS_pred']), 3)\n",
    "    df.at['MAE (in avg)', 'Wind']= round(mae(data['Target'], data['WS_pred']), 3)\n",
    "    df.at['MAPE (%)', 'Wind']= round(mape(data['Target'], data['WS_pred'])*100, 3)\n",
    "    \n",
    "    #df.at['RMSE (as root mean)', 'Power']= round(rmse(data['P'], data['P_pred']), 3)\n",
    "    df.at['MAE (in avg)', 'Power']= round(mae(data['P'], data['P_pred']), 3)\n",
    "    df.at['MAPE (%)', 'Power']= round(mape(data['P'], data['P_pred'])*100, 3)\n",
    "    \n",
    "    \n",
    "    print('Wind RMSE: ', round(rmse(data['Target'], data['WS_pred']), 3), 'm/s as root mean')\n",
    "    print('Wind MAE: ', round(mae(data['Target'], data['WS_pred']), 3), 'm/s in avg')\n",
    "    print('Wind MAPE: ', round(mape(data['Target'], data['WS_pred'])*100, 3), '%')\n",
    "    \n",
    "    print('Power RMSE: ', round(rmse(data['P'], data['P_pred']), 3), 'kW as root mean')\n",
    "    print('Power MAE: ', round(mae(data['P'], data['P_pred']), 3), 'kW in avg')\n",
    "    print('Power MAPE: ', round(mape(data['P'], data['P_pred'])*100, 3), '%')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f9f017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_plot(data, title):\n",
    "    \n",
    "    #title is expected to be an str\n",
    "    #WS_pred and Target should be the variables names\n",
    "\n",
    "    #plotting the reference\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot([-1,17.5],[-1,17.5], 'green', linewidth=4, alpha=.12)\n",
    "    plt.plot(data['WS_pred'], data['Target'], marker='o', ls='', label='Regression', markersize=5, alpha=.1)\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    ax=plt.gca()\n",
    "    ax.set(xlabel='y predicted', ylabel='y actual');\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylim(ymin=4, ymax=17.5)\n",
    "    ax.set_xlim(xmin=4, xmax=17.5)\n",
    "    \n",
    "    return print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58bd13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powercurve_computation(data, power_curve):\n",
    "    \n",
    "    from scipy import interpolate\n",
    "    \n",
    "    #this function computes the power at a observation given the information at a observation:\n",
    "    # the WS (in m/s) at the wind turbine location and at the hub height (Target)\n",
    "    # the power curve of the wind turbine in an xslx\n",
    "    \n",
    "    \n",
    "    x=power_curve['Wind Speed [m/s]']\n",
    "    y=power_curve['Warranted Power Curve [kW]']\n",
    "    x_new=data['Target']\n",
    "    \n",
    "    f = interpolate.interp1d(x, y)\n",
    "    #, kind='linear'\n",
    "    data['P']=f(x_new)\n",
    "    \n",
    "    if 'WS_pred' in data.keys():\n",
    "        x_new2=data['WS_pred']\n",
    "        data['P_pred']=f(x_new2)\n",
    "    \n",
    "    print('power curve computation performed')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c09062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_power_computation (data_test, data_train, power_curve):\n",
    "    \n",
    "    results_test=pd.DataFrame()\n",
    "    results_train=pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    results_test=powercurve_computation(data_test, power_curve)\n",
    "    results_train=powercurve_computation(data_train, power_curve)\n",
    "\n",
    "    return results_test, results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3896d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(data_test, data_train, power_curve, plot_error):\n",
    "    \n",
    "    #this function computes and plots the results of a modelling:\n",
    "\n",
    "    results_test, results_train=control_power_computation (data_test, data_train, power_curve)\n",
    "    \n",
    "    \n",
    "    print('Modelling errors for training set:')\n",
    "    errors_computation(results_train)\n",
    "    print('')\n",
    "    print('Modelling errors for test set:')\n",
    "    errors_computation(results_test)\n",
    "    \n",
    "    if plot_error:\n",
    "        print('')\n",
    "        error_plot(results_test, 'Error plot for test set wind speed')\n",
    "\n",
    "    print('')\n",
    "    return print('Showing the results of the modelling: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71970e76",
   "metadata": {},
   "source": [
    "## Data uploading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bcfb73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uploading_csv(file_folder,file_name):\n",
    "    \n",
    "    #file folder required\n",
    "    #file name required\n",
    "    #file is expected to be in the data root: r'C:\\Users\\irgaa\\Irma\\Data'\n",
    "    #this function uploads and formats csv/txt/xlsx datasets into DataFrame\n",
    "    \n",
    "    \n",
    "    data_root=r'C:\\Users\\irgaa\\Irma\\Data'\n",
    "    data_folder=str(file_folder)\n",
    "    data_file=str(file_name)\n",
    "    \n",
    "    data_path=data_root+data_folder+data_file\n",
    "    \n",
    "    data1 = pd.read_csv(data_path)\n",
    "\n",
    "    \n",
    "    # We will save the WD_bin as the index\n",
    "    \n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c5e33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function saves a data csv\n",
    "\n",
    "def save (data, file_folder,file_name):\n",
    "    \n",
    "    #file folder required\n",
    "    #file name required\n",
    "    #file is expected to be saved in the data root: r'C:\\Users\\irgaa\\Irma\\Data'\n",
    "    #this function saves a csv/txt/xlsx into Irma's folder\n",
    "    #the saved file will keep the columns names but not the index\n",
    "    \n",
    "    data_root=r'C:\\Users\\irgaa\\Irma\\Data'\n",
    "    data_folder=str(file_folder)\n",
    "    data_file=str(file_name)\n",
    "    \n",
    "    data_path=data_root+data_folder+data_file\n",
    "    \n",
    "    data.to_csv (data_path, index = False, header=True)\n",
    "    \n",
    "    \n",
    "    return print('file', data_file, 'saved in', data_folder, 'folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaedaca",
   "metadata": {},
   "source": [
    "## Data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "358a5d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_selection(X_train, X_test, inputs):\n",
    "    \n",
    "    #this function returns the columns of the training and test sets in the inputs list\n",
    "    \n",
    "    X_train1 = pd.DataFrame()\n",
    "    X_test1 = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    X_train1 = X_train[inputs]\n",
    "    X_test1 = X_test[inputs]\n",
    "\n",
    "    \n",
    "    return X_train1,X_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fd717ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_drop(X_train, X_test, list_2drop):\n",
    "    \n",
    "    #this function returns the columns of the training and test sets in the inputs list\n",
    "\n",
    "    X_train1 = X_train.drop(columns=list_2drop)\n",
    "    X_test1 = X_test.drop(columns=list_2drop)\n",
    "\n",
    "    \n",
    "    \n",
    "    return X_train1,X_test1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c2b31",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d5c814",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6eec63a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model (n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8],\n",
    "                 activation='relu', optimizer='Adam', regularization=None, Leaky=False):\n",
    "    \n",
    "    #this function build model is created only for building the NN\n",
    "    #it will always initialize the weights using the strategy 'He normal' unless activation function 'selu' is selected\n",
    "    \n",
    "    \n",
    "    if activation!='relu':\n",
    "        Leaky==False\n",
    "    \n",
    "    \n",
    "    #we create a sequential model:\n",
    "    model=keras.models.Sequential()\n",
    "    \n",
    "    #we add the input layer with n_neurons=n_features (shape of X_train)\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    #we add a hidden layer for each n_hidden with ReLU as activation function\n",
    "    #this code considers the same n_neurons for each hidden layer\n",
    "    for layer in range(n_hidden):\n",
    "        \n",
    "        if regularization=='l2':\n",
    "            if activation=='relu':\n",
    "                model.add(keras.layers.Dense(n_neurons, activation='relu', kernel_initializer='he_normal',\n",
    "                                            kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "            elif activation=='elu':\n",
    "                model.add(keras.layers.Dense(n_neurons, activation='elu', kernel_initializer='he_normal',\n",
    "                                            kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "            elif activation=='selu':\n",
    "                model.add(keras.layers.Dense(n_neurons, activation='selu', kernel_initializer='lecun_normal',\n",
    "                                           kernel_regularizer=keras.regularizers.l2(0.01)))  \n",
    "        elif regularization=='l1':\n",
    "            if activation=='relu':\n",
    "                model.add(keras.layers.Dense(n_neurons, activation='relu', kernel_initializer='he_normal',\n",
    "                                             kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "            elif activation=='elu':\n",
    "                model.add(keras.layers.Dense(n_neurons, activation='elu', kernel_initializer='he_normal',\n",
    "                                             kernel_regularizer=keras.regularizers.l1(0.01)))\n",
    "            elif activation=='selu':\n",
    "                model.add(keras.layers.Dense(n_neurons, activation='selu', kernel_initializer='lecun_normal',\n",
    "                                            kernel_regularizer=keras.regularizers.l1(0.01))) \n",
    "        else:\n",
    "            if activation=='relu':\n",
    "                model.add(keras.layers.Dense(n_neurons, activation='relu', kernel_initializer='he_normal'))\n",
    "            elif activation=='elu':\n",
    "                model.add(keras.layers.Dense(n_neurons, activation='elu', kernel_initializer='he_normal'))\n",
    "            elif activation=='selu':\n",
    "                model.add(keras.layers.Dense(n_neurons, activation='selu', kernel_initializer='lecun_normal'))\n",
    "        \n",
    "        if Leaky:\n",
    "            model.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    #dropout only considered in the last layer\n",
    "    if regularization=='Dropout':\n",
    "        model.add(keras.layers.Dropout(rate=0.2))\n",
    "        \n",
    "    #we add the output layer with one neuron (we only want to predict 1 target)\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    #we choose our optimizer and build it:\n",
    "    if optimizer=='SGD':\n",
    "        optimizer=keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer=='Momentum':\n",
    "        optimizer=keras.optimizers.SGD(lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer=='Nesterov':\n",
    "        optimizer=keras.optimizers.SGD(lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "    elif optimizer=='RMSprop':\n",
    "        optimizer=keras.optimizers.RMSprop(lr=learning_rate, rho=0.9)\n",
    "    elif optimizer=='Adam':\n",
    "        optimizer=keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "    elif optimizer=='Nadam':\n",
    "        optimizer=keras.optimizers.Nadam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "    \n",
    "    #we compile our model with the selected optimizer and set the objective function loss='mse'\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb2847dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_old (n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8],\n",
    "                 activation='relu', optimizer='Adam', regularization=None, Leaky=False):\n",
    "    \n",
    "    #this function build model is created only for building the NN\n",
    "    #it will always initialize the weights using the strategy 'He normal' unless activation function 'selu' is selected\n",
    "    \n",
    "    \n",
    "    if activation!='relu':\n",
    "        Leaky==False\n",
    "    \n",
    "    \n",
    "    #we create a sequential model:\n",
    "    model=keras.models.Sequential()\n",
    "    \n",
    "    #we add the input layer with n_neurons=n_features (shape of X_train)\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    \n",
    "    #we add a hidden layer for each n_hidden with ReLU as activation function\n",
    "    #this code considers the same n_neurons for each hidden layer\n",
    "    for layer in range(n_hidden):\n",
    "        if activation=='relu':\n",
    "            model.add(keras.layers.Dense(n_neurons, activation='relu', kernel_initializer='he_normal'))\n",
    "        elif activation=='elu':\n",
    "            model.add(keras.layers.Dense(n_neurons, activation='elu', kernel_initializer='he_normal'))\n",
    "        elif activation=='selu':\n",
    "            model.add(keras.layers.Dense(n_neurons, activation='selu', kernel_initializer='lecun_normal'))\n",
    "        if Leaky:\n",
    "            model.add(keras.layers.LeakyReLU(alpha=0.2))\n",
    "            \n",
    "    if regularization=='Dropout':\n",
    "        model.add(keras.layers.Dropout(rate=0.2))\n",
    "        \n",
    "    #we add the output layer with one neuron (we only want to predict 1 target)\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    #we choose our optimizer and build it:\n",
    "    if optimizer=='SGD':\n",
    "        optimizer=keras.optimizers.SGD(lr=learning_rate)\n",
    "    elif optimizer=='Momentum':\n",
    "        optimizer=keras.optimizers.SGD(lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer=='Nesterov':\n",
    "        optimizer=keras.optimizers.SGD(lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "    elif optimizer=='RMSprop':\n",
    "        optimizer=keras.optimizers.RMSprop(lr=learning_rate, rho=0.9)\n",
    "    elif optimizer=='Adam':\n",
    "        optimizer=keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "    elif optimizer=='Nadam':\n",
    "        optimizer=keras.optimizers.Nadam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "    \n",
    "    #we compile our model with the selected optimizer and set the objective function loss='mse'\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983f849",
   "metadata": {},
   "source": [
    "### Modelling NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c827782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling_NN (X, X_test, y, y_test, power_curve,  parameters, plot_error, plot):\n",
    "    \n",
    "    #creating the model\n",
    "    \n",
    "    n_hidden=parameters['n_hidden']\n",
    "    n_neurons=parameters['n_neurons']\n",
    "    learning_rate=parameters['learning_rate']\n",
    "    input_shape=X.shape[1:]\n",
    "    activation=parameters['activation']\n",
    "    optimizer=parameters['optimizer']\n",
    "    regularization=parameters['regularization']\n",
    "    Leaky=parameters['Leaky']\n",
    "    \n",
    "    model =build_model(n_hidden, n_neurons, learning_rate, input_shape,\n",
    "                 activation, optimizer, regularization, Leaky)\n",
    " \n",
    "    \n",
    "    #model fitting\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "    \n",
    "\n",
    "    if regularization=='Early Stopping':\n",
    "        model.fit(X_train, y_train, epochs=100,\n",
    "                      validation_data=(X_valid, y_valid),\n",
    "                      callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])\n",
    "    else:\n",
    "        model.fit(X_train, y_train, epochs=100,\n",
    "                      validation_data=(X_valid, y_valid))\n",
    "    \n",
    "    \n",
    "#     if plot:\n",
    "#         loss_epochs_plot (history)\n",
    "        \n",
    "    \n",
    "    #model predicting\n",
    "    y_pred_test=model.predict(X_test)\n",
    "    y_pred_train=model.predict(X)\n",
    "    \n",
    "    test=pd.DataFrame(y_pred_test, columns = ['test'])\n",
    "    train=pd.DataFrame(y_pred_train, columns = ['train'])\n",
    "    \n",
    "    \n",
    "\n",
    "    #computing the results\n",
    "    data_test = pd.DataFrame()\n",
    "    data_train = pd.DataFrame()\n",
    "    \n",
    "    data_test['WS_pred']=test['test']\n",
    "    data_test['Target']=y_test['Target']\n",
    "    data_train['WS_pred']=train['train']\n",
    "    data_train['Target']=y['Target']\n",
    "    \n",
    "#     mse_test=model.evaluate(X_test, y_test)\n",
    "#     rmse_test=np.sqrt(mse_test)\n",
    "#     print('RMSE for test', rmse_test)\n",
    "    \n",
    "    \n",
    "    compute_results(data_test, data_train, power_curve, plot_error)\n",
    "    print('NN modelling performed')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f6df93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling_NN_ES (X, X_test, y, y_test, power_curve,  parameters, plot_error, plot):\n",
    "    \n",
    "    #creating the model\n",
    "    \n",
    "    n_hidden=parameters['n_hidden']\n",
    "    n_neurons=parameters['n_neurons']\n",
    "    learning_rate=parameters['learning_rate']\n",
    "    input_shape=X.shape[1:]\n",
    "    activation=parameters['activation']\n",
    "    optimizer=parameters['optimizer']\n",
    "    regularization=parameters['regularization']\n",
    "    Leaky=parameters['Leaky']\n",
    "    \n",
    "    model =build_model(n_hidden, n_neurons, learning_rate, input_shape,\n",
    "                 activation, optimizer, regularization, Leaky)\n",
    " \n",
    "    \n",
    "    #model fitting\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "    \n",
    "\n",
    "    model.fit(X_train, y_train, epochs=100,\n",
    "                      validation_data=(X_valid, y_valid),\n",
    "                      callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])\n",
    "    \n",
    "    \n",
    "#     if plot:\n",
    "#         loss_epochs_plot (history)\n",
    "        \n",
    "    \n",
    "    #model predicting\n",
    "    y_pred_test=model.predict(X_test)\n",
    "    y_pred_train=model.predict(X)\n",
    "    \n",
    "    test=pd.DataFrame(y_pred_test, columns = ['test'])\n",
    "    train=pd.DataFrame(y_pred_train, columns = ['train'])\n",
    "    \n",
    "    \n",
    "\n",
    "    #computing the results\n",
    "    data_test = pd.DataFrame()\n",
    "    data_train = pd.DataFrame()\n",
    "    \n",
    "    data_test['WS_pred']=test['test']\n",
    "    data_test['Target']=y_test['Target']\n",
    "    data_train['WS_pred']=train['train']\n",
    "    data_train['Target']=y['Target']\n",
    "    \n",
    "#     mse_test=model.evaluate(X_test, y_test)\n",
    "#     rmse_test=np.sqrt(mse_test)\n",
    "#     print('RMSE for test', rmse_test)\n",
    "    \n",
    "    \n",
    "    compute_results(data_test, data_train, power_curve, plot_error)\n",
    "    print('NN modelling performed')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c283c5",
   "metadata": {},
   "source": [
    "### Random Search NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c6cd04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomSearch_NN(X, X_test, y, y_test, power_curve, param_distribs, plot_error):\n",
    "    \n",
    "    #counting the runing time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    #creating the model\n",
    "    input_shape=X.shape[1:]\n",
    "    param_distribs['input_shape']=input_shape\n",
    "    keras_reg =keras.wrappers.scikit_learn.KerasRegressor(build_model, verbose=0)\n",
    "    \n",
    "    \n",
    "    #Random Search CV\n",
    "    rnd_search_cv=RandomizedSearchCV(keras_reg, param_distribs, n_iter=20, cv=3)\n",
    "    \n",
    "    #model fitting\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    regularization=param_distribs['regularization']\n",
    "    \n",
    "    if regularization=='Early Stopping':\n",
    "        rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                      validation_data=(X_valid, y_valid),\n",
    "                      callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])\n",
    "    else:\n",
    "        rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                      validation_data=(X_valid, y_valid))\n",
    "    \n",
    "    \n",
    "    #model predicting\n",
    "    \n",
    "    model=rnd_search_cv.best_estimator_.model\n",
    "    y_pred_test=model.predict(X_test)\n",
    "    y_pred_train=model.predict(X)\n",
    "    \n",
    "    test=pd.DataFrame(y_pred_test, columns = ['test'])\n",
    "    train=pd.DataFrame(y_pred_train, columns = ['train'])\n",
    "    \n",
    "    print('')\n",
    "    print('Best parameters :')\n",
    "    print(rnd_search_cv.best_params_)\n",
    "    print('')\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "    print('')\n",
    "\n",
    "    #computing the results\n",
    "    data_test = pd.DataFrame()\n",
    "    data_train = pd.DataFrame()\n",
    "    \n",
    "    data_test['WS_pred']=test['test']\n",
    "    data_test['Target']=y_test['Target']\n",
    "    data_train['WS_pred']=train['train']\n",
    "    data_train['Target']=y['Target']\n",
    "    \n",
    "    compute_results(data_test, data_train, power_curve, plot_error)\n",
    "    print('RandomSearch_ NN performed')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6523601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomSearch_NN_ES(X, X_test, y, y_test, power_curve, param_distribs, plot_error):\n",
    "    \n",
    "    #creating the model\n",
    "    input_shape=X.shape[1:]\n",
    "    param_distribs['input_shape']=input_shape\n",
    "    keras_reg =keras.wrappers.scikit_learn.KerasRegressor(build_model, verbose=0)\n",
    "    \n",
    "    #Random Search CV\n",
    "    rnd_search_cv=RandomizedSearchCV(keras_reg, param_distribs, n_iter=20, cv=3)\n",
    "    \n",
    "    #model fitting\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                      validation_data=(X_valid, y_valid),\n",
    "                      callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])\n",
    "    \n",
    "    \n",
    "    #model predicting\n",
    "    \n",
    "    model=rnd_search_cv.best_estimator_.model\n",
    "    y_pred_test=model.predict(X_test)\n",
    "    y_pred_train=model.predict(X)\n",
    "    \n",
    "    test=pd.DataFrame(y_pred_test, columns = ['test'])\n",
    "    train=pd.DataFrame(y_pred_train, columns = ['train'])\n",
    "    \n",
    "    print('')\n",
    "    print('Best parameters :')\n",
    "    print(rnd_search_cv.best_params_)\n",
    "    print('')\n",
    "\n",
    "    #computing the results\n",
    "    data_test = pd.DataFrame()\n",
    "    data_train = pd.DataFrame()\n",
    "    \n",
    "    data_test['WS_pred']=test['test']\n",
    "    data_test['Target']=y_test['Target']\n",
    "    data_train['WS_pred']=train['train']\n",
    "    data_train['Target']=y['Target']\n",
    "    \n",
    "    compute_results(data_test, data_train, power_curve, plot_error)\n",
    "    print('RandomSearch_ NN performed')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7836fc",
   "metadata": {},
   "source": [
    "### Loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a862833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_epochs_plot (history):\n",
    "    \n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "    plt.grid(True)\n",
    "    plt.show\n",
    "    \n",
    "    return print('Loss vs. epochs plot performed')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54004cbb",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bfe8458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_testing (X_train, X_test, y_train, y_test, power_curve, model, plot_error):\n",
    "\n",
    "    \n",
    "\n",
    "    y_pred_test=model.predict(X_test)\n",
    "    y_pred_train=model.predict(X_train)\n",
    "    \n",
    "    test=pd.DataFrame(y_pred_test, columns = ['test'])\n",
    "    train=pd.DataFrame(y_pred_train, columns = ['train'])\n",
    "\n",
    "\n",
    "    data_test = pd.DataFrame()\n",
    "    data_train = pd.DataFrame()\n",
    "    \n",
    "    data_test['WS_pred']=test['test']\n",
    "    data_test['Target']=y_test['Target']\n",
    "    data_train['WS_pred']=train['train']\n",
    "    data_train['Target']=y_train['Target']\n",
    "    \n",
    "    plot_model(model, show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    \n",
    "    compute_results(data_test, data_train, power_curve, plot_error)\n",
    "    \n",
    "    WS_pred=data_test['WS_pred']\n",
    "    print('NN results performed')\n",
    "    \n",
    "    return WS_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724a96a",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51189ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance (X_train, X_test, model):\n",
    "    \n",
    "    X_t, X_f, y_t, y_f = train_test_split(X_train,y_train, test_size=0.02, random_state=12)\n",
    "    \n",
    "    background = X_f.copy()\n",
    "    \n",
    "    explainer = shap.KernelExplainer(model.predict,background)\n",
    "    shap_values = explainer.shap_values(X_test,nsamples=100)\n",
    "    shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
    "    print('Feature importance through SHAP values performed')\n",
    "    \n",
    "\n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "046028e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_shap (shap_values, X_test):\n",
    "\n",
    "    v=np.array(shap_values)\n",
    "    d=v.reshape(X_test.shape)\n",
    "    shap_v=pd.DataFrame(d)\n",
    "    \n",
    "    feature_list=X_test.columns\n",
    "    shap_v.columns=feature_list\n",
    "    shap_v=shap_v.abs()\n",
    "    k=pd.DataFrame(shap_v.mean()).reset_index()\n",
    "    k.columns=['variables','SHAP_abs']\n",
    "    k.sort_values(by='variables')\n",
    "    \n",
    "    return k\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d0819",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0fec4a",
   "metadata": {},
   "source": [
    "## Dataset1- WTG14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3f2c12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['T2', 'RH2', 'T1', 'RH1', 'PR1', 'AD1', 'PR2', 'AD2', 'Rain', 'WS1',\n",
       "       'WS3', 'WS4', 'WD1', 'WD3', 'WD4', 'WSHor', 'WDHor', 'WSVer', 'WDVer',\n",
       "       'TI', 'WSH', 'WD_bin', 'tod', 'WVeer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upload the dataset with file_folder, file_name\n",
    "# data_up= uploading_csv('\\Dataset1-Normal_Site','\\data_comp14.csv')\n",
    "X_train= uploading_csv('\\Dataset1-Normal_Site','\\X_train14.csv')\n",
    "X_test= uploading_csv('\\Dataset1-Normal_Site','\\X_test14.csv')\n",
    "y_train= uploading_csv('\\Dataset1-Normal_Site','\\y_train14.csv')\n",
    "y_test= uploading_csv('\\Dataset1-Normal_Site','\\y_test14.csv')\n",
    "\n",
    "X_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d0a26d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Target'], dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08af60ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T2</th>\n",
       "      <th>RH2</th>\n",
       "      <th>T1</th>\n",
       "      <th>RH1</th>\n",
       "      <th>PR1</th>\n",
       "      <th>AD1</th>\n",
       "      <th>PR2</th>\n",
       "      <th>AD2</th>\n",
       "      <th>Rain</th>\n",
       "      <th>WS1</th>\n",
       "      <th>...</th>\n",
       "      <th>WD4</th>\n",
       "      <th>WSHor</th>\n",
       "      <th>WDHor</th>\n",
       "      <th>WSVer</th>\n",
       "      <th>WDVer</th>\n",
       "      <th>TI</th>\n",
       "      <th>WSH</th>\n",
       "      <th>WD_bin</th>\n",
       "      <th>tod</th>\n",
       "      <th>WVeer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.317472</td>\n",
       "      <td>0.636355</td>\n",
       "      <td>0.340125</td>\n",
       "      <td>0.619360</td>\n",
       "      <td>0.030801</td>\n",
       "      <td>0.484003</td>\n",
       "      <td>0.035362</td>\n",
       "      <td>0.498223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534138</td>\n",
       "      <td>0.400754</td>\n",
       "      <td>0.715190</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>0.492834</td>\n",
       "      <td>0.401251</td>\n",
       "      <td>0.316505</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.083916</td>\n",
       "      <td>0.655683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.089466</td>\n",
       "      <td>0.791074</td>\n",
       "      <td>0.101354</td>\n",
       "      <td>0.809109</td>\n",
       "      <td>0.737198</td>\n",
       "      <td>0.870019</td>\n",
       "      <td>0.765898</td>\n",
       "      <td>0.881242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.490117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.722105</td>\n",
       "      <td>0.494487</td>\n",
       "      <td>0.804378</td>\n",
       "      <td>0.562792</td>\n",
       "      <td>0.485612</td>\n",
       "      <td>0.198639</td>\n",
       "      <td>0.310161</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.223776</td>\n",
       "      <td>0.497300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.556667</td>\n",
       "      <td>0.587084</td>\n",
       "      <td>0.572726</td>\n",
       "      <td>0.600075</td>\n",
       "      <td>0.736106</td>\n",
       "      <td>0.420494</td>\n",
       "      <td>0.744780</td>\n",
       "      <td>0.421262</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.664341</td>\n",
       "      <td>0.513474</td>\n",
       "      <td>0.714333</td>\n",
       "      <td>0.550759</td>\n",
       "      <td>0.500389</td>\n",
       "      <td>0.456993</td>\n",
       "      <td>0.286590</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>0.503073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.135653</td>\n",
       "      <td>0.864725</td>\n",
       "      <td>0.132842</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.205901</td>\n",
       "      <td>0.721012</td>\n",
       "      <td>0.203882</td>\n",
       "      <td>0.712520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748591</td>\n",
       "      <td>0.227576</td>\n",
       "      <td>0.988457</td>\n",
       "      <td>0.416675</td>\n",
       "      <td>0.386403</td>\n",
       "      <td>0.425696</td>\n",
       "      <td>0.186749</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.377622</td>\n",
       "      <td>0.597797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.369032</td>\n",
       "      <td>0.359721</td>\n",
       "      <td>0.386156</td>\n",
       "      <td>0.351401</td>\n",
       "      <td>0.707787</td>\n",
       "      <td>0.587475</td>\n",
       "      <td>0.725928</td>\n",
       "      <td>0.595351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601639</td>\n",
       "      <td>0.486262</td>\n",
       "      <td>0.644355</td>\n",
       "      <td>0.418298</td>\n",
       "      <td>0.416080</td>\n",
       "      <td>0.255090</td>\n",
       "      <td>0.377160</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.811189</td>\n",
       "      <td>0.520563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         T2       RH2        T1       RH1       PR1       AD1       PR2  \\\n",
       "0  0.317472  0.636355  0.340125  0.619360  0.030801  0.484003  0.035362   \n",
       "1  0.089466  0.791074  0.101354  0.809109  0.737198  0.870019  0.765898   \n",
       "2  0.556667  0.587084  0.572726  0.600075  0.736106  0.420494  0.744780   \n",
       "3  0.135653  0.864725  0.132842  0.884058  0.205901  0.721012  0.203882   \n",
       "4  0.369032  0.359721  0.386156  0.351401  0.707787  0.587475  0.725928   \n",
       "\n",
       "        AD2  Rain       WS1  ...       WD4     WSHor     WDHor     WSVer  \\\n",
       "0  0.498223   0.0  0.389831  ...  0.534138  0.400754  0.715190  0.554293   \n",
       "1  0.881242   0.0  0.490117  ...  0.722105  0.494487  0.804378  0.562792   \n",
       "2  0.421262   0.0  0.510292  ...  0.664341  0.513474  0.714333  0.550759   \n",
       "3  0.712520   0.0  0.219129  ...  0.748591  0.227576  0.988457  0.416675   \n",
       "4  0.595351   0.0  0.478077  ...  0.601639  0.486262  0.644355  0.418298   \n",
       "\n",
       "      WDVer        TI       WSH  WD_bin       tod     WVeer  \n",
       "0  0.492834  0.401251  0.316505     0.8  0.083916  0.655683  \n",
       "1  0.485612  0.198639  0.310161     0.8  0.223776  0.497300  \n",
       "2  0.500389  0.456993  0.286590     0.8  0.860140  0.503073  \n",
       "3  0.386403  0.425696  0.186749     1.0  0.377622  0.597797  \n",
       "4  0.416080  0.255090  0.377160     0.6  0.811189  0.520563  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db055157",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC= uploading_csv('\\Dataset1-Normal_Site','\\PC_1.15kgm-3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34adb853",
   "metadata": {},
   "source": [
    "### RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ff683ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs={\n",
    "    'n_hidden':[0, 1, 2, 3],\n",
    "    'n_neurons': [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'learning_rate': [0.0001, 0.0003, 0.001, 0.003, 0.005, 0.01, 0.03],\n",
    "    'activation':['relu', 'elu', 'selu'],\n",
    "    'optimizer':['Adam', 'Nesterov', 'Momentum', 'Nadam', 'RMSProp'],\n",
    "    'regularization':[None, 'Dropout', 'Early Stopping'],\n",
    "    'Leaky':[True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74963bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [-3.98551663 -0.14395947 -0.32253063         nan -0.15152594 -0.22173306\n",
      " -0.66491832 -0.22325017 -0.15306451 -0.21406976 -0.1724265  -1.19458397\n",
      " -0.2154945  -0.16120041 -0.18491064 -0.18464693 -0.1743432  -0.2270332\n",
      " -0.17390798 -0.20609822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters :\n",
      "{'regularization': None, 'optimizer': 'Adam', 'n_neurons': 90, 'n_hidden': 3, 'learning_rate': 0.005, 'input_shape': 24, 'activation': 'selu', 'Leaky': False}\n",
      "\n",
      "--- 8.844393825531006 minutes ---\n",
      "\n",
      "power curve computation performed\n",
      "power curve computation performed\n",
      "Modelling errors for training set:\n",
      "Wind RMSE:  0.364 m/s as root mean\n",
      "Wind MAE:  0.281 m/s in avg\n",
      "Wind MAPE:  3.265 %\n",
      "Power RMSE:  163.174 kW as root mean\n",
      "Power MAE:  103.878 kW in avg\n",
      "Power MAPE:  7.198 %\n",
      "\n",
      "Modelling errors for test set:\n",
      "Wind RMSE:  0.405 m/s as root mean\n",
      "Wind MAE:  0.31 m/s in avg\n",
      "Wind MAPE:  3.671 %\n",
      "Power RMSE:  187.912 kW as root mean\n",
      "Power MAE:  118.157 kW in avg\n",
      "Power MAPE:  8.383 %\n",
      "\n",
      "Showing the results of the modelling: \n",
      "RandomSearch_ NN performed\n"
     ]
    }
   ],
   "source": [
    "model= RandomSearch_NN(X_train, X_test, y_train, y_test, PC, param_distribs, plot_error=False)\n",
    "#iter=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd3b8850",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs={\n",
    "    'n_hidden':[0, 1, 2, 3],\n",
    "    'n_neurons': [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'learning_rate': [0.0001, 0.0003, 0.001, 0.003, 0.005, 0.01, 0.03],\n",
    "    'activation':['relu', 'elu', 'selu'],\n",
    "    'optimizer':['Adam', 'Nesterov', 'Momentum', 'Nadam', 'RMSProp'],\n",
    "    'regularization':[None, 'Dropout', 'Early Stopping'],\n",
    "    'Leaky':[True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9055db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters :\n",
      "{'regularization': None, 'optimizer': 'Nesterov', 'n_neurons': 80, 'n_hidden': 1, 'learning_rate': 0.005, 'input_shape': 25, 'activation': 'relu', 'Leaky': False}\n",
      "\n",
      "--- 8.028279749552409 minutes ---\n",
      "\n",
      "power curve computation performed\n",
      "power curve computation performed\n",
      "Modelling errors for training set:\n",
      "Wind RMSE:  0.355 m/s as root mean\n",
      "Wind MAE:  0.269 m/s in avg\n",
      "Wind MAPE:  3.166 %\n",
      "Power RMSE:  157.018 kW as root mean\n",
      "Power MAE:  97.824 kW in avg\n",
      "Power MAPE:  7.235 %\n",
      "\n",
      "Modelling errors for test set:\n",
      "Wind RMSE:  0.38 m/s as root mean\n",
      "Wind MAE:  0.287 m/s in avg\n",
      "Wind MAPE:  3.395 %\n",
      "Power RMSE:  170.91 kW as root mean\n",
      "Power MAE:  108.358 kW in avg\n",
      "Power MAPE:  7.832 %\n",
      "\n",
      "Showing the results of the modelling: \n",
      "RandomSearch_ NN performed\n"
     ]
    }
   ],
   "source": [
    "model= RandomSearch_NN(X_train, X_test, y_train, y_test, PC, param_distribs, plot_error=False)\n",
    "#iter=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a8cb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs={\n",
    "    'n_hidden':[0, 1, 2, 3],\n",
    "    'n_neurons': [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'learning_rate': [0.0001, 0.0003, 0.001, 0.003, 0.005, 0.01, 0.03],\n",
    "    'activation':['relu', 'elu', 'selu'],\n",
    "    'optimizer':['Adam', 'Nesterov', 'Momentum', 'Nadam', 'RMSProp'],\n",
    "    'regularization':[None, 'Dropout'],\n",
    "    'Leaky':[True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03ebe239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters :\n",
      "{'regularization': None, 'optimizer': 'Nesterov', 'n_neurons': 80, 'n_hidden': 1, 'learning_rate': 0.01, 'input_shape': 25, 'activation': 'relu', 'Leaky': False}\n",
      "\n",
      "power curve computation performed\n",
      "power curve computation performed\n",
      "Modelling errors for training set:\n",
      "Wind RMSE:  0.384 m/s as root mean\n",
      "Wind MAE:  0.293 m/s in avg\n",
      "Wind MAPE:  3.396 %\n",
      "Power RMSE:  165.7 kW as root mean\n",
      "Power MAE:  104.965 kW in avg\n",
      "Power MAPE:  7.47 %\n",
      "\n",
      "Modelling errors for test set:\n",
      "Wind RMSE:  0.395 m/s as root mean\n",
      "Wind MAE:  0.298 m/s in avg\n",
      "Wind MAPE:  3.501 %\n",
      "Power RMSE:  175.827 kW as root mean\n",
      "Power MAE:  112.186 kW in avg\n",
      "Power MAPE:  7.905 %\n",
      "\n",
      "Showing the results of the modelling: \n",
      "RandomSearch_ NN performed\n"
     ]
    }
   ],
   "source": [
    "model= RandomSearch_NN_ES(X_train, X_test, y_train, y_test, PC, param_distribs, plot_error=False)\n",
    "#iter=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "66bc751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs={\n",
    "    'n_hidden':[0, 1, 2, 3],\n",
    "    'n_neurons': [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'learning_rate': [0.0001, 0.0003, 0.001, 0.003, 0.005, 0.01, 0.03],\n",
    "    'activation':['relu', 'elu', 'selu'],\n",
    "    'optimizer':['Adam', 'Nesterov', 'Momentum', 'Nadam', 'RMSProp'],\n",
    "    'regularization':['iRMA', 'Dropout'],\n",
    "    'Leaky':[True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a78b31b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters :\n",
      "{'regularization': 'Dropout', 'optimizer': 'Nesterov', 'n_neurons': 100, 'n_hidden': 2, 'learning_rate': 0.005, 'input_shape': 25, 'activation': 'relu', 'Leaky': False}\n",
      "\n",
      "power curve computation performed\n",
      "power curve computation performed\n",
      "Modelling errors for training set:\n",
      "Wind RMSE:  0.432 m/s as root mean\n",
      "Wind MAE:  0.326 m/s in avg\n",
      "Wind MAPE:  3.827 %\n",
      "Power RMSE:  183.918 kW as root mean\n",
      "Power MAE:  114.768 kW in avg\n",
      "Power MAPE:  8.683 %\n",
      "\n",
      "Modelling errors for test set:\n",
      "Wind RMSE:  0.44 m/s as root mean\n",
      "Wind MAE:  0.331 m/s in avg\n",
      "Wind MAPE:  3.9 %\n",
      "Power RMSE:  190.303 kW as root mean\n",
      "Power MAE:  120.777 kW in avg\n",
      "Power MAPE:  8.91 %\n",
      "\n",
      "Showing the results of the modelling: \n",
      "RandomSearch_ NN performed\n"
     ]
    }
   ],
   "source": [
    "model= RandomSearch_NN_ES(X_train, X_test, y_train, y_test, PC, param_distribs, plot_error=False)\n",
    "#iter=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f3cb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9fefea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs={\n",
    "    'n_hidden':[0, 1, 2, 3],\n",
    "    'n_neurons': [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'learning_rate': [0.0001, 0.0003, 0.001, 0.003, 0.005, 0.01, 0.03],\n",
    "    'activation':['relu', 'elu', 'selu'],\n",
    "    'optimizer':['Adam', 'Nesterov', 'Momentum', 'Nadam', 'RMSProp'],\n",
    "    'regularization':[None, 'Dropout', 'Early Stopping'],\n",
    "    'Leaky':[True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "76cf5471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters :\n",
      "{'regularization': 'Early Stopping', 'optimizer': 'Adam', 'n_neurons': 80, 'n_hidden': 2, 'learning_rate': 0.003, 'input_shape': 25, 'activation': 'elu', 'Leaky': True}\n",
      "\n",
      "--- 8.597513794898987 minutes ---\n",
      "\n",
      "power curve computation performed\n",
      "power curve computation performed\n",
      "Modelling errors for training set:\n",
      "Wind RMSE:  0.416 m/s as root mean\n",
      "Wind MAE:  0.339 m/s in avg\n",
      "Wind MAPE:  3.78 %\n",
      "Power RMSE:  183.277 kW as root mean\n",
      "Power MAE:  122.235 kW in avg\n",
      "Power MAPE:  7.603 %\n",
      "\n",
      "Modelling errors for test set:\n",
      "Wind RMSE:  0.446 m/s as root mean\n",
      "Wind MAE:  0.357 m/s in avg\n",
      "Wind MAPE:  4.076 %\n",
      "Power RMSE:  198.688 kW as root mean\n",
      "Power MAE:  132.408 kW in avg\n",
      "Power MAPE:  8.569 %\n",
      "\n",
      "Showing the results of the modelling: \n",
      "RandomSearch_ NN performed\n"
     ]
    }
   ],
   "source": [
    "model= RandomSearch_NN(X_train, X_test, y_train, y_test, PC, param_distribs, plot_error=False)\n",
    "#iter=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b483daf",
   "metadata": {},
   "source": [
    "### Manual modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4145fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={\n",
    "    'n_hidden':3,\n",
    "    'n_neurons': 80,\n",
    "    'learning_rate':0.001,\n",
    "    'activation':'elu',\n",
    "    'optimizer':'Adam',\n",
    "    'regularization':None,\n",
    "    'Leaky':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3e2970c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 11.5409 - val_loss: 1.0413\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4488 - val_loss: 0.3178\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2776 - val_loss: 0.2382\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.2392 - val_loss: 0.2300\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.2258 - val_loss: 0.2252\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2290 - val_loss: 0.2034\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.2096 - val_loss: 0.2152\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.2052 - val_loss: 0.2174\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1941 - val_loss: 0.1959\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1882 - val_loss: 0.2341\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1892 - val_loss: 0.1976\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1834 - val_loss: 0.1692\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1851 - val_loss: 0.2365\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1842 - val_loss: 0.1880\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1736 - val_loss: 0.1919\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1772 - val_loss: 0.1954\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1706 - val_loss: 0.1580\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1626 - val_loss: 0.2030\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1721 - val_loss: 0.1808\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1580 - val_loss: 0.2831\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1597 - val_loss: 0.1575\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1599 - val_loss: 0.2357\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1532 - val_loss: 0.1456\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1462 - val_loss: 0.1861\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1539 - val_loss: 0.1374\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1469 - val_loss: 0.1561\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1643 - val_loss: 0.1424\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1446 - val_loss: 0.1372\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.1520 - val_loss: 0.1645\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.1575 - val_loss: 0.1373\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.1389 - val_loss: 0.1797\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.1438 - val_loss: 0.1493\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1448 - val_loss: 0.1676\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1412 - val_loss: 0.1485\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.1442 - val_loss: 0.1545\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1425 - val_loss: 0.4419\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1611 - val_loss: 0.1286\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1325 - val_loss: 0.1316\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.1328 - val_loss: 0.1444\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.1370 - val_loss: 0.1430\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1335 - val_loss: 0.1619\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1278 - val_loss: 0.1637\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1355 - val_loss: 0.1210\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1515 - val_loss: 0.2002\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1449 - val_loss: 0.1812\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.1359 - val_loss: 0.1711\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.1342 - val_loss: 0.1255\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.1312 - val_loss: 0.1570\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.1337 - val_loss: 0.1380\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.1301 - val_loss: 0.1337\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.1366 - val_loss: 0.1256\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1190 - val_loss: 0.1568\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1245 - val_loss: 0.1556\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1224 - val_loss: 0.1215\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.2395\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1481 - val_loss: 0.1322\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1281 - val_loss: 0.1459\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1404 - val_loss: 0.1230\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.1316 - val_loss: 0.1232\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.1196 - val_loss: 0.1230\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.1260 - val_loss: 0.1287\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.1267 - val_loss: 0.1297\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1310 - val_loss: 0.1299\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1342 - val_loss: 0.1326\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 0.1288 - val_loss: 0.1247\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1163 - val_loss: 0.1242\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1205 - val_loss: 0.1151\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1132 - val_loss: 0.1196\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1139 - val_loss: 0.1151\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1175 - val_loss: 0.1202\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1150 - val_loss: 0.1268\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1182 - val_loss: 0.1185\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1257 - val_loss: 0.1296\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1205 - val_loss: 0.1407\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1223 - val_loss: 0.1314\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1137 - val_loss: 0.1163\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1299 - val_loss: 0.1481\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1146 - val_loss: 0.1359\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1161 - val_loss: 0.1350\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1187 - val_loss: 0.2046\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1137 - val_loss: 0.1254\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1275 - val_loss: 0.1617\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1207 - val_loss: 0.1341\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1197 - val_loss: 0.1290\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1192 - val_loss: 0.1131\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1130 - val_loss: 0.1259\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1148 - val_loss: 0.1198\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.1117 - val_loss: 0.1175\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1109 - val_loss: 0.1111\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1153 - val_loss: 0.1207\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1160 - val_loss: 0.1364\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1125 - val_loss: 0.1151\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.1154 - val_loss: 0.1127\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.1016 - val_loss: 0.1557\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1308 - val_loss: 0.1309\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1108 - val_loss: 0.1121\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1165 - val_loss: 0.1684\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1102 - val_loss: 0.1103\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1033 - val_loss: 0.1122\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1251 - val_loss: 0.1176\n",
      "power curve computation performed\n",
      "power curve computation performed\n",
      "Modelling errors for training set:\n",
      "Wind RMSE:  0.323 m/s as root mean\n",
      "Wind MAE:  0.249 m/s in avg\n",
      "Wind MAPE:  2.877 %\n",
      "Power RMSE:  146.898 kW as root mean\n",
      "Power MAE:  92.022 kW in avg\n",
      "Power MAPE:  6.187 %\n",
      "\n",
      "Modelling errors for test set:\n",
      "Wind RMSE:  0.363 m/s as root mean\n",
      "Wind MAE:  0.277 m/s in avg\n",
      "Wind MAPE:  3.262 %\n",
      "Power RMSE:  167.626 kW as root mean\n",
      "Power MAE:  106.493 kW in avg\n",
      "Power MAPE:  7.32 %\n",
      "\n",
      "Showing the results of the modelling: \n",
      "NN modelling performed\n"
     ]
    }
   ],
   "source": [
    "model = modelling_NN (X_train, X_test, y_train, y_test, PC, parameters, plot_error=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7377f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={\n",
    "    'n_hidden':3,\n",
    "    'n_neurons': 80,\n",
    "    'learning_rate':0.001,\n",
    "    'activation':'elu',\n",
    "    'optimizer':'Adam',\n",
    "    'regularization':None,\n",
    "    'Leaky':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1906f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 19.4191 - val_loss: 1.6904\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5838 - val_loss: 0.3369\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2454 - val_loss: 0.2113\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.2232 - val_loss: 0.2005\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.2049 - val_loss: 0.2050\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1960 - val_loss: 0.1898\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1891 - val_loss: 0.2013\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1864 - val_loss: 0.1858\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1828 - val_loss: 0.2010\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1833 - val_loss: 0.1736\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1823 - val_loss: 0.1746\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1822 - val_loss: 0.1983\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1799 - val_loss: 0.2088\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1785 - val_loss: 0.1701\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1741 - val_loss: 0.1756\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1784 - val_loss: 0.2756\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1871 - val_loss: 0.1646\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1664 - val_loss: 0.1639\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1640 - val_loss: 0.1639\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1616 - val_loss: 0.1844\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1667 - val_loss: 0.1601\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1556 - val_loss: 0.1554\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1559 - val_loss: 0.1755\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1609 - val_loss: 0.2458\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1579 - val_loss: 0.1752\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1678 - val_loss: 0.2555\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1620 - val_loss: 0.1581\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1515 - val_loss: 0.1652\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1499 - val_loss: 0.1519\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1494 - val_loss: 0.1468\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1575 - val_loss: 0.1387\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1379 - val_loss: 0.1440\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1409 - val_loss: 0.1353\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1365 - val_loss: 0.1474\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1450 - val_loss: 0.1668\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1396 - val_loss: 0.1493\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1406 - val_loss: 0.1862\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1398 - val_loss: 0.1363\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1429 - val_loss: 0.1464\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1460 - val_loss: 0.2203\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1501 - val_loss: 0.1348\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1247 - val_loss: 0.1678\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1511 - val_loss: 0.1368\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1404 - val_loss: 0.1970\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1325 - val_loss: 0.1314\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1322 - val_loss: 0.1325\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1277 - val_loss: 0.1274\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1266 - val_loss: 0.1458\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1393 - val_loss: 0.1860\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1325 - val_loss: 0.1282\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1291 - val_loss: 0.1467\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1329 - val_loss: 0.1300\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1273 - val_loss: 0.1332\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1277 - val_loss: 0.1268\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1350 - val_loss: 0.1349\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1237 - val_loss: 0.1376\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1345 - val_loss: 0.1227\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1200 - val_loss: 0.1315\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1236 - val_loss: 0.1642\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1283 - val_loss: 0.1207\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1160 - val_loss: 0.1446\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1304 - val_loss: 0.1339\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1173 - val_loss: 0.1296\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1251 - val_loss: 0.1146\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1220 - val_loss: 0.1219\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1167 - val_loss: 0.1169\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1200 - val_loss: 0.1179\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1188 - val_loss: 0.1971\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1318 - val_loss: 0.1464\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1336 - val_loss: 0.1574\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1183 - val_loss: 0.1284\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1193 - val_loss: 0.1236\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1148 - val_loss: 0.1243\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1294 - val_loss: 0.1649\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1154 - val_loss: 0.1249\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1167 - val_loss: 0.1213\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1102 - val_loss: 0.1437\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1229 - val_loss: 0.1135\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1127 - val_loss: 0.1308\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1180 - val_loss: 0.1131\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1186 - val_loss: 0.3861\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1339 - val_loss: 0.1142\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1081 - val_loss: 0.1107\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1094 - val_loss: 0.2276\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1164 - val_loss: 0.1118\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1177 - val_loss: 0.1113\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1084 - val_loss: 0.1191\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1148 - val_loss: 0.1183\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 0.1118 - val_loss: 0.1116\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.1193 - val_loss: 0.1457\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1043 - val_loss: 0.1148\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.1153 - val_loss: 0.1142\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1041 - val_loss: 0.1799\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 0.1231\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 0.2133\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 0.1261\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1016 - val_loss: 0.1087\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1156 - val_loss: 0.1138\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1028 - val_loss: 0.1600\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1082 - val_loss: 0.1467\n",
      "power curve computation performed\n",
      "power curve computation performed\n",
      "Modelling errors for training set:\n",
      "Wind RMSE:  0.359 m/s as root mean\n",
      "Wind MAE:  0.281 m/s in avg\n",
      "Wind MAPE:  3.186 %\n",
      "Power RMSE:  160.88 kW as root mean\n",
      "Power MAE:  102.797 kW in avg\n",
      "Power MAPE:  6.678 %\n",
      "\n",
      "Modelling errors for test set:\n",
      "Wind RMSE:  0.398 m/s as root mean\n",
      "Wind MAE:  0.31 m/s in avg\n",
      "Wind MAPE:  3.588 %\n",
      "Power RMSE:  185.623 kW as root mean\n",
      "Power MAE:  119.201 kW in avg\n",
      "Power MAPE:  7.848 %\n",
      "\n",
      "Showing the results of the modelling: \n",
      "NN modelling performed\n"
     ]
    }
   ],
   "source": [
    "model = modelling_NN (X_train, X_test, y_train, y_test, PC, parameters, plot_error=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c2ee2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={\n",
    "    'n_hidden':3,\n",
    "    'n_neurons': 80,\n",
    "    'learning_rate':0.001,\n",
    "    'activation':'elu',\n",
    "    'optimizer':'Adam',\n",
    "    'regularization':None,\n",
    "    'Leaky':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "54351cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 11.1970 - val_loss: 0.5620\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.3371 - val_loss: 0.2971\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2650 - val_loss: 0.2868\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2347 - val_loss: 0.2271\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.2124 - val_loss: 0.2134\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - ETA: 0s - loss: 0.2031- ETA: 0s - loss: 0.20 - 0s 2ms/step - loss: 0.2025 - val_loss: 0.1901\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1990 - val_loss: 0.2390\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1977 - val_loss: 0.1808\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.1975 - val_loss: 0.3261\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 1s 7ms/step - loss: 0.1947 - val_loss: 0.1819\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.1789 - val_loss: 0.1717\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1788 - val_loss: 0.1971\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 0.1727 - val_loss: 0.2056\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 0.1731 - val_loss: 0.1712\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.1720 - val_loss: 0.1659\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 1s 7ms/step - loss: 0.1655 - val_loss: 0.1686\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.1660 - val_loss: 0.1652\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 0s 5ms/step - loss: 0.1603 - val_loss: 0.1883\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1647 - val_loss: 0.1766\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 0.1549 - val_loss: 0.1660\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1595 - val_loss: 0.1647\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1569 - val_loss: 0.1803\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1615 - val_loss: 0.1696\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1567 - val_loss: 0.1633\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1593 - val_loss: 0.1605\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1577 - val_loss: 0.1644\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1745 - val_loss: 0.1799\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1488 - val_loss: 0.1592\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1515 - val_loss: 0.1463\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1440 - val_loss: 0.1602\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1446 - val_loss: 0.1517\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1472 - val_loss: 0.1839\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1478 - val_loss: 0.1480\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1420 - val_loss: 0.1536\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1499 - val_loss: 0.1525\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1424 - val_loss: 0.1549\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1376 - val_loss: 0.1622\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1462 - val_loss: 0.1594\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1330 - val_loss: 0.1432\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1429 - val_loss: 0.1423\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1274 - val_loss: 0.1405\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1359 - val_loss: 0.3152\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1489 - val_loss: 0.2215\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1363 - val_loss: 0.1524\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1379 - val_loss: 0.1538\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1363 - val_loss: 0.1366\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1550 - val_loss: 0.1641\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1294 - val_loss: 0.1340\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1342 - val_loss: 0.2034\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1331 - val_loss: 0.1313\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1226 - val_loss: 0.1320\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1228 - val_loss: 0.1543\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1306 - val_loss: 0.1368\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1358 - val_loss: 0.3686\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1357 - val_loss: 0.1233\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1386 - val_loss: 0.1323\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1303 - val_loss: 0.2340\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1292 - val_loss: 0.1384\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1325 - val_loss: 0.1472\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1178 - val_loss: 0.1254\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1195 - val_loss: 0.1277\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 0s 4ms/step - loss: 0.1172 - val_loss: 0.1268\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.1201 - val_loss: 0.1243\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1222 - val_loss: 0.2273\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.1337 - val_loss: 0.1337\n",
      "power curve computation performed\n",
      "power curve computation performed\n",
      "Modelling errors for training set:\n",
      "Wind RMSE:  0.338 m/s as root mean\n",
      "Wind MAE:  0.257 m/s in avg\n",
      "Wind MAPE:  3.004 %\n",
      "Power RMSE:  151.645 kW as root mean\n",
      "Power MAE:  94.689 kW in avg\n",
      "Power MAPE:  6.769 %\n",
      "\n",
      "Modelling errors for test set:\n",
      "Wind RMSE:  0.362 m/s as root mean\n",
      "Wind MAE:  0.275 m/s in avg\n",
      "Wind MAPE:  3.265 %\n",
      "Power RMSE:  164.115 kW as root mean\n",
      "Power MAE:  104.378 kW in avg\n",
      "Power MAPE:  7.536 %\n",
      "\n",
      "Showing the results of the modelling: \n",
      "NN modelling performed\n"
     ]
    }
   ],
   "source": [
    "model = modelling_NN_ES (X_train, X_test, y_train, y_test, PC, parameters, plot_error=False, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb2d2b",
   "metadata": {},
   "source": [
    "### Model saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8524f1c",
   "metadata": {},
   "source": [
    "parameters={\n",
    "    'n_hidden':3,\n",
    "    'n_neurons': 80,\n",
    "    'learning_rate':0.001,\n",
    "    'activation':'elu',\n",
    "    'optimizer':'Adam',\n",
    "    'regularization':None,\n",
    "    'Leaky':True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97c207f",
   "metadata": {},
   "source": [
    "MAPE power: 7.32 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1f39b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('WTG14_ANN1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc6c76b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "645b0ff8",
   "metadata": {},
   "source": [
    "MAPE power: xx %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab98c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('WTG14_Axx.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a46ffb",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ffdeaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model('WTG14_ANN1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd3e3741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "power curve computation performed\n",
      "power curve computation performed\n",
      "Modelling errors for training set:\n",
      "Wind RMSE:  0.323 m/s as root mean\n",
      "Wind MAE:  0.249 m/s in avg\n",
      "Wind MAPE:  2.877 %\n",
      "Power RMSE:  146.898 kW as root mean\n",
      "Power MAE:  92.022 kW in avg\n",
      "Power MAPE:  6.187 %\n",
      "\n",
      "Modelling errors for test set:\n",
      "Wind RMSE:  0.363 m/s as root mean\n",
      "Wind MAE:  0.277 m/s in avg\n",
      "Wind MAPE:  3.262 %\n",
      "Power RMSE:  167.626 kW as root mean\n",
      "Power MAE:  106.493 kW in avg\n",
      "Power MAPE:  7.32 %\n",
      "\n",
      "Showing the results of the modelling: \n",
      "NN results performed\n"
     ]
    }
   ],
   "source": [
    "WS_pred=model_testing (X_train, X_test, y_train, y_test, PC, model, plot_error=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93f8623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file ANN_WTG14.csv saved in \\Results_ folder\n"
     ]
    }
   ],
   "source": [
    "WS_pred=pd.DataFrame(WS_pred)\n",
    "save(WS_pred,'\\Results_','ANN_WTG14.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b99bf60",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "18076b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63e54a9f78740678ab7b424eff06f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgIAAAI0CAYAAABvZkF8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABd/UlEQVR4nO3de1hU1cIG8BcchdDCBAULQ4/A2EVRGOeGqEDmFe+WZaVZomYKapkdP49aHjU1FRKtrJNmkeW1RDIj0cMIqTOSQIghmsdCwAsqowID7O8Pc+soNy/MBvb7e56enLX37Fl7rWHmnbX2xU4QBAFEREQkS/ZSV4CIiIikwyBAREQkYwwCREREMsYgQEREJGMMAkRERDLGIEBERCRjsgsCSUlJUldB9n777TepqyB77ANpsf2lxz64QXZBwMHBQeoqyF5RUZHUVZA99oG02P7SYx/cILsgQERERDcwCBAREckYgwAREZGMMQgQERHJGIMAERGRjDEIEBERyRiDABERkYwxCBAREckYgwAREZGMMQgQERHJGIMAERGRjDEIEBERyRiDABERkYwxCBAREcmYnSAIgtSVsCW7paVSV4GIiKhSwpsKm74eRwSIiIhkjEGAiIhIxhgEiIiIZOyOg0B4eDgiIyOtyiZNmgSNRoPCwkKxLCUlBYGBgTCbzVi0aBH69OmDbt26YeDAgYiKikJJSQkAICcnByqVCleuXLHa5pUrV6BSqZCTk3M3+0VEREQ1cMdBQKvVIiUlRXx89epVpKWlwcvLC8nJyWK50WiESqXCkiVLcObMGcTExMBgMCA6OhpGoxErVqy4LztAREREd++ugkBmZiaKiooAAAcOHIBSqURISAgMBoO4ntFohF6vR0ZGBrp3744WLVoAANq0aYNp06bhoYceuqPXPXLkCMLCwtCjRw8MGzYM27dvF5eFhobi3//+N0JCQrBw4cI73SUiIiLZuuNzFNq1awcXFxekpqZCrVbDYDAgICAAWq0WMTExKC8vh8ViQXp6OmbPno2CggIsW7YMR48ehVqtRqdOndC5c2d07tzZarv9+vWr9DULCgowceJETJgwAatWrUJmZibCw8PRokULBAQEAAByc3OxY8cOlJby9EAiIqKauquTFa9PD6jVaiQlJWH58uXw9vaGQqFARkYGioqK4ObmBg8PD4SFhcHLywuxsbGYO3cuzGYzfH19MWPGDCiVSnGbcXFxcHJyEh9fuXIF3bt3BwDs3bsXbm5uGDlyJADgqaeewpAhQxAbGysGgeDgYDg6Ot51QxAREcnRXQeBzZs349ixYxAEAT4+PgAAnU6H/fv3w2KxQK/Xi+sHBwcjODgY5eXlyMrKwtq1azF58mSr4f2qFBQUoHXr1lZl7u7uVscquLi43M2uEBER1Skmk+m+b9Pf37/SZXcVBNRqNebPnw+DwWD1hR8QEIBt27ahpKQEY8aMQX5+PoYMGYINGzagTZs2sLe3h1KpxMyZMxESEoKzZ8/Czs6u2tdzd3fHDz/8YFWWk5MjHncAoEbbISIiquuq+tKuDXd1HQFnZ2d4enpiy5YtVkFAq9UiKysL2dnZ8PPzQ6tWrdCxY0csWLAAJ06cAHDt1/3atWvh7e1926/8ygQEBOD8+fPYsGEDSktLkZ6ejm3btqFv3753U30iIiL6211fUEin0yE/Px8ajUYsa9asGTw9PfHEE0+I8/VLly6Fl5cXwsPD0a1bNwwfPhznzp1DVFQU7O1r9vIPPfQQPvzwQ8THxyMkJASzZs3CG2+8geDg4LutPhEREYE3HSIiIqpTeNMhIiIishkGASIiIhljECAiIpIx2R0jYDKZbH5qBlljH0iPfSAttr/02Ac3cESAiIhIxhgEiIiIZIxBgIiISMYYBIiIiGSMQYCIiEjGGASIiIhkjEGAiIhIxmR3HQHea4CISBq2voZ+VXgdgRs4IkBERCRjDAJEREQyxiBAREQkYzYJAuHh4YiMjLQqmzRpEjQaDQoLC8WylJQUBAYGwmw2Y9GiRejTpw+6deuGgQMHIioqCiUlJbdt+/jx4wgICMCxY8dqfT+IiIgaGpsEAa1Wi5SUFPHx1atXkZaWBi8vLyQnJ4vlRqMRKpUKS5YswZkzZxATEwODwYDo6GgYjUasWLHCarulpaWYM2cOiouLbbEbREREDY7NgkBmZiaKiooAAAcOHIBSqURISAgMBoO4ntFohF6vR0ZGBrp3744WLVoAANq0aYNp06bhoYcestru6tWr0bVrV1vsAhERUYNkkyDQrl07uLi4IDU1FQBgMBgQEBAAvV6PpKQklJeXo7i4GOnp6dDpdOjVqxeWLVuGxYsXY8+ePTh//jw6d+6MCRMmiNtMSUlBcnIyJk6caItdICIiapBsdlLn9ekBtVqNpKQkLF++HN7e3lAoFMjIyEBRURHc3Nzg4eGBsLAweHl5ITY2FnPnzoXZbIavry9mzJgBpVIJs9mM9957D4sWLULjxo1ttQtERHQPTCaT1FWwUtfqU5uqumaCTYPA5s2bcezYMQiCAB8fHwCATqfD/v37YbFYoNfrxfWDg4MRHByM8vJyZGVlYe3atZg8eTK2b9+OJUuWIDQ0VNwGERHVfXXpAj68oNANNjt9UK1W48iRIzAYDFZf+AEBAUhJScGhQ4eg1+uRn5+PgIAAnDp16loF7e2hVCoxc+ZMnD9/HmfPnkV8fDzWrVuHnj17omfPngCAV199FTt37rTV7hARETUINgsCzs7O8PT0xJYtW6yCgFarRVZWFrKzs+Hn54dWrVqhY8eOWLBgAU6cOAEAKCgowNq1a+Ht7Y3WrVtj37592LNnj/gfAHz22Wfo06ePrXaHiIioQbDpBYV0Oh3y8/Oh0WjEsmbNmsHT0xNPPPEEHB0dAQBLly6Fl5cXwsPD0a1bNwwfPhznzp1DVFQU7O15DSQiIqL7hTcdIiIim+BNh+om/rwmIiKSMQYBIiIiGWMQICIikrG6M2FjI8agw5wXkhjn5qTHPpAW25/qEo4IEBERyRiDABERkYwxCBAREckYgwAREZGMMQgQERHJGIMAERGRjPESw0REVK26dHng+4GncN7AEQEiIiIZYxAgIiKSMQYBIiIiGbNJEAgPD0dkZKRV2aRJk6DRaFBYWCiWpaSkIDAwEGazGYsWLUKfPn3QrVs3DBw4EFFRUSgpKblt2wkJCXj55ZdrfR+IiIgaIpsEAa1Wi5SUFPHx1atXkZaWBi8vLyQnJ4vlRqMRKpUKS5YswZkzZxATEwODwYDo6GgYjUasWLFCXLe0tBTr1q3DrFmzILPjHYmIiO4bmwWBzMxMFBUVAQAOHDgApVKJkJAQGAwGcT2j0Qi9Xo+MjAx0794dLVq0AAC0adMG06ZNw0MPPSSuu2jRIuzbtw+jRo2yxS4QERE1SDYJAu3atYOLiwtSU1MBAAaDAQEBAdDr9UhKSkJ5eTmKi4uRnp4OnU6HXr16YdmyZVi8eDH27NmD8+fPo3PnzpgwYYK4zfHjx+OTTz7BY489ZotdICIiapBsdrDgzdMDSUlJ0Ov1UCqVUCgUyMjIQFpaGtzc3ODh4YGwsDDMmTMHubm5mDt3Lp555hm8+uqrOHr0qLi9li1b2qrqREREDZbNrhCh1WqxefNmHDt2DIIgwMfHBwCg0+mwf/9+WCwW6PV6cf3g4GAEBwejvLwcWVlZWLt2LSZPnozt27fDwcHBVtUmIiJcuwBPQ9MQ96kyVV08yWZBQK1WY/78+TAYDFZf+AEBAdi2bRtKSkowZswY5OfnY8iQIdiwYQPatGkDe3t7KJVKzJw5EyEhITh79iweffRRW1WbiIhQ9RdJfcQrC95gs6kBZ2dneHp6YsuWLVZBQKvVIisrC9nZ2fDz80OrVq3QsWNHLFiwACdOnAAAFBQUYO3atfD29kbr1q1tVWUiIqIGz6YXFNLpdMjPz4dGoxHLmjVrBk9PTzzxxBNwdHQEACxduhReXl4IDw9Ht27dMHz4cJw7dw5RUVGwt+c1kIiIiO4X3nSIiIiqxZsONVz8eU1ERCRjDAJEREQyxiBAREQkYw1r0qcGjEGHOS8kMc7NSY99IC22P9UlHBEgIiKSMQYBIiIiGWMQICIikjEGASIiIhljECAiIpIxBgEiIiIZYxAgIiKSMdldR0CV4Ask8H4DtaWhXY+ciKih44gAERGRjDEIEBERyRiDABERkYzV6QndKVOmICUlBQBQUlICOzs7NG7cGADQt29f/POf/wQAJCQk4PPPP8cXX3whWV2JiIjqozodBKKiosR/z5gxA+3bt8f48ePFstLSUnz11Vf4+OOP0b59eymqSEREVK/V66mBRYsWYd++fRg1apTUVSEiIqqX6nUQGD9+PD755BM89thjUleFiIioXqrTUwPVadmypdRVoFuYTKb7uh7VHvaBtNj+0pNTH/j7+1e6rF4HAap7qnqzXWcymWq0HtUe9oG02P7SYx/cUK+nBoiIiOjeMAgQERHJGIMAERGRjNWbYwQWL15c6bLQ0FCEhobasDZEREQNA0cEiIiIZIxBgIiISMYYBIiIiGSs3hwjcL8Ygw7z3FEiIqK/cUSAiIhIxhgEiIiIZIxBgIiISMYYBIiIiGSMQYCIiEjGGASIiIhkTHanD6oSfIGEUqmrUacJb8rubUFEJFscESAiIpIxBgEiIiIZYxAgIiKSMQYBIiIiGavTR4VNmTIFKSkpAICSkhLY2dmhcePGAIC+fftCr9dj9erVyM3NhZubGyZOnIigoCApq0xERFSv1OkgEBUVJf57xowZaN++PcaPHw8AOHnyJF5++WUsXboUKpUK+/fvx1tvvYX169ejbdu2EtWYiIiofqm3UwOnT5/G4MGD0bVrV9jZ2UGr1cLT0xPp6elSV42IiKjeqNMjAlXRarXQarXi4z///BPHjx+Hj4+PhLUiIiKqX+ptELjZmTNnEB4ejgEDBjAI3Acmk6lBvAZVjX0gLba/9OTUB/7+/pUuq/dBIDMzE9OmTUO3bt0wc+ZMqavTIFT1hrkfTCZTrb8GVY19IC22v/TYBzfU6yCQlJSEd955B+PGjcOLL74odXWIiIjqnXobBLKzszFjxgzMnj0bvXv3lro6RERE9VK9PWtgw4YNKC4uxvz58xEYGCj+t2XLFqmrRkREVG/UmxGBxYsXWz2eNWsWZs2aJVFtiIiIGoZ6OyJARERE945BgIiISMYYBIiIiGSs3hwjcL8Ygw7z3FEiIqK/cUSAiIhIxhgEiIiIZIxBgIiISMYYBIiIiGSMQYCIiEjGGASIiIhkTHanD6oSfIGEUqmrcd8Ib8quC4mI6D7iiAAREZGMMQgQERHJGIMAERGRjNVqEAgPD0dkZKRV2aRJk6DRaFBYWCiWpaSkIDAwEGazGYsWLUKfPn3QrVs3DBw4EFFRUSgpKQEA5OTkQKVS4cqVK1bbvHLlClQqFXJycmpzd4iIiBqcWg0CWq0WKSkp4uOrV68iLS0NXl5eSE5OFsuNRiNUKhWWLFmCM2fOICYmBgaDAdHR0TAajVixYkVtVpOIiEi2aj0IZGZmoqioCABw4MABKJVKhISEwGAwiOsZjUbo9XpkZGSge/fuaNGiBQCgTZs2mDZtGh566KHarCYREZFs1eq5Z+3atYOLiwtSU1OhVqthMBgQEBAArVaLmJgYlJeXw2KxID09HbNnz0ZBQQGWLVuGo0ePQq1Wo1OnTujcuTM6d+5std1+/frVZrWJiIhko9ZPQr8+PaBWq5GUlITly5fD29sbCoUCGRkZKCoqgpubGzw8PBAWFgYvLy/ExsZi7ty5MJvN8PX1xYwZM6BUKsVtxsXFwcnJSXx85coVdO/evbZ3hYiIqMGxSRDYvHkzjh07BkEQ4OPjAwDQ6XTYv38/LBYL9Hq9uH5wcDCCg4NRXl6OrKwsrF27FpMnT8b27dtru6r1kslkkroKd6W+1rshYR9Ii+0vPTn1gb+/f6XLaj0IqNVqzJ8/HwaDweoLPyAgANu2bUNJSQnGjBmD/Px8DBkyBBs2bECbNm1gb28PpVKJmTNnIiQkBGfPnoWdnV1tV7feqapz6yqTyVQv692QsA+kxfaXHvvghlq/joCzszM8PT2xZcsWqyCg1WqRlZWF7Oxs+Pn5oVWrVujYsSMWLFiAEydOAAAKCgqwdu1aeHt7o3Xr1rVdVSIiItmxyQWFdDod8vPzodFoxLJmzZrB09MTTzzxBBwdHQEAS5cuhZeXF8LDw9GtWzcMHz4c586dQ1RUFOztee0jIiKi+81OEARB6krYkt3ShnPDIaB+3nSIQ3LSYx9Ii+0vPfbBDfyZTUREJGMMAkRERDLGIEBERCRj9W+C+R4Zgw5zXoiIiOhvHBEgIiKSMQYBIiIiGWMQICIikjEGASIiIhljECAiIpIxBgEiIiIZYxAgIiKSMdldR0CV4Ask1N/7DdTHewsQEVHdxREBIiIiGWMQICIikjEGASIiIhmTZMJZpVLBwcEB9vbXcoggCGjZsiVGjx6NwYMHAwDCwsIQEhKC5557zuq5M2bMQPv27TF+/HiUlpYiMjISu3btgsViga+vL95++224u7vbepeIiIjqJclGBNatW4fExEQkJiZi7969CAsLw4IFC3DixIkab+Ozzz7Db7/9hpiYGPzwww9o2bIlZs2aVYu1JiIialjqxNRAo0aN0LdvXzRt2hTZ2dk1fl5RURFee+01uLi4wMHBAc8++yzS09NRXl5ei7UlIiJqOOrEuWgWiwUbN26ExWJBx44dxfKoqCisXr3aat2ioiK0b98eABAeHm61bO/evWjfvr045UBERERVkywIvPrqqwCuhQAA0Ol0+Oijj+Dm5iauM2XKlAqPEajIrl278PnnnyMyMrKWalw3mEwmqatwXzSU/ajP2AfSYvtLT0594O/vX+kyyYLAZ599Bi8vL/z1119466230Lx5czz55JN3ta21a9fi888/x+LFi6vc2YagIeyfyWRqEPtRn7EPpMX2lx774AbJx9AfffRRfPDBB9i9ezf+85//3NFzy8vLMX/+fGzatAlr1qyBXq+vpVoSERE1TJIHAQBo3bo1pk2bhjVr1iArK6vGz1uzZg0OHjyItWvXwsfHpxZrSERE1DDViYMFASA0NBQ7d+7Eu+++i7Vr11a7fmlpKdavX4/S0lIMGTLEatmuXbvwwAMP1FJNiYiIGg5JgoDRaKywPDo6Wvz3J598UuE6ixcvFv9tMBjub8WIiIhkpk5MDRAREZE0GASIiIhkjEGAiIhIxurMwYK2Ygw6zHNHiYiI/sYRASIiIhljECAiIpIxBgEiIiIZYxAgIiKSMQYBIiIiGWMQICIikjHZnT6oSvAFEkqlrkaFhDdl1x1ERCQxjggQERHJGIMAERGRjDEIEBERyZhNJqXDw8Pxj3/8A+Hh4WLZpEmTYDQaER8fjwcffBAAkJKSgilTpsDb2xtHjhyBQnGtegqFAr6+vpg8eTLat29/2/aXLl0KhUKBiIgIW+wOERFRg2GTEQGtVouUlBTx8dWrV5GWlgYvLy8kJyeL5UajESqVSvxST0xMRGJiImJjY6FUKhEWFoa8vDxx/QsXLmDu3LnYsGGDLXaDiIiowbFZEMjMzERRUREA4MCBA1AqlQgJCYHBYBDXMxqN0Ov1tz2/adOmmDhxIry8vBATEyOWv/baa2jUqBGCg4NrfyeIiIgaIJsEgXbt2sHFxQWpqakAAIPBgICAAOj1eiQlJaG8vBzFxcVIT0+HTqerdDs6nQ6//vqr+Hj16tWYPXs2nJycansXiIiIGiSbHSx48/RAUlIS9Ho9lEolFAoFMjIykJaWBjc3N3h4eFS6DWdnZ5jNZvFxy5Yta73eREREDZnNrmCj1WqxefNmHDt2DIIgwMfHB8C1X/n79++HxWKpcFrgZhcuXIC7u7stqisJk8kkdRVsRk77WlexD6TF9peenPrA39+/0mU2CwJqtRrz58+HwWCw+sIPCAjAtm3bUFJSgjFjxlS5jeTkZHTq1KmWayqdqjqqITGZTLLZ17qKfSAttr/02Ac32GxqwNnZGZ6entiyZYtVENBqtcjKykJ2djb8/PwqfK7ZbEZ0dDROnjyJkSNH2qrKREREDZ5NL26v0+mwbt06aDQasaxZs2bw9PSEg4MDHB0dxfIVK1Zg5cqVsLOzg5OTE7p06YI1a9bA1dXVllUmIiJq0OwEQRCkroQt2S2tmzccAuRz0yEOyUmPfSAttr/02Ac38BLDREREMsYgQEREJGMMAkRERDImj0npmxiDDnNeiIiI6G8cESAiIpIxBgEiIiIZYxAgIiKSMQYBIiIiGWMQICIikjEGASIiIhljECAiIpIx2V1HQJXgCyTY9n4DcrmHABER1T8cESAiIpIxBgEiIiIZYxAgIiKSMQYBIiIiGasyCISHhyMyMtKqbNKkSdBoNCgsLBTLUlJSEBgYiLFjx0Kn0yEwMBCBgYEICgpCREQEsrOza1QZo9GIkJCQSpcHBgbixIkTNdoWERERVa/KIKDVapGSkiI+vnr1KtLS0uDl5YXk5GSx3Gg0QqVSQaFQICIiAomJiUhMTERsbCyUSiXCwsKQl5d3z5VNTExEu3bt7nk7REREdE21QSAzMxNFRUUAgAMHDkCpVCIkJAQGg0Fcz2g0Qq/X3/b8pk2bYuLEifDy8kJMTEyNKiQIAiIjI9GrVy8MHz4c8fHx4jKVSoVjx44hJycHPXv2xNq1a9G7d2/06tULH3zwQY22T0RERDdUeYJ7u3bt4OLigtTUVKjVahgMBgQEBECr1SImJgbl5eWwWCxIT0/H7Nmz8dNPP1W4HZ1Oh4SEhBpV6NKlSwCAHTt24NChQ5g2bRq8vLzQtm1bq/XMZjNycnKwfft2HD16FGFhYejVqxc6depUo9exJZPJJHUV6hy2ifTYB9Ji+0tPTn3g7+9f6bJqr3RzfXpArVYjKSkJy5cvh7e3NxQKBTIyMlBUVAQ3Nzd4eHhUug1nZ2eYzeYaVdbJyQmvv/46GjduDK1WC51Oh/j4eLz22mu3rTt69Gg0adIEHTt2RNu2bfG///2vTgaBqjpAjkwmE9tEYuwDabH9pcc+uKFGQWDz5s04duwYBEGAj48PgGu/8vfv3w+LxVLhtMDNLly4AHd39xpVyNXVFY0bNxYft2rVCmfPnq1w3YcffvjGjigUEAShRq9BRERE11R7+qBarcaRI0dgMBisvvADAgKQkpKCQ4cOVRsEkpOT8fjjj9eoQgUFBSgrKxMf5+bm1jhEEBER0Z2pNgg4OzvD09MTW7ZssfrC12q1yMrKQnZ2Nvz8/Cp8rtlsRnR0NE6ePImRI0fWqEKFhYX47LPPUFJSgsTERJhMJvTp06eGu0NERER3okZ3w9HpdFi3bh00Go1Y1qxZM3h6esLBwQGOjo5i+YoVK7By5UrY2dnByckJXbp0wZo1a+Dq6lqjCrVp0wb5+fl4+umn0bp1ayxZsoQjAkRERLXETpDZxLrdUtveeRDg3QdvxYN0pMc+kBbbX3rsgxt4iWEiIiIZs+lP1eXLl2PLli2VLk9MTLRhbYiIiMimQWDq1KmYOnWqLV/yNsagwxwOIiIi+hunBoiIiGSMQYCIiEjGGASIiIhkjEGAiIhIxhgEiIiIZIxBgIiISMZkd8k7VYIvkFD7Vxfk1QSJiKg+4IgAERGRjDEIEBERyRiDABERkYwxCBAREcnYXQWB8PBwREZGWpVNmjQJGo0GhYWFYllKSgoCAwNhNpuxaNEi9OnTB926dcPAgQMRFRWFkpISCIKAIUOG4Msvv7ztdS5fvozAwECYTKa7qSYRERFV466CgFarRUpKivj46tWrSEtLg5eXF5KTk8Vyo9EIlUqFJUuW4MyZM4iJiYHBYEB0dDSMRiNWrFgBOzs7DBo0CDt27LjtdX766Se4u7vzJkFERES15K6DQGZmJoqKigAABw4cgFKpREhICAwGg7ie0WiEXq9HRkYGunfvjhYtWgAA2rRpg2nTpuGhhx4CAAwcOBAnTpzA0aNHrV7n+++/x9ChQwEAu3fvxrPPPouePXti4sSJOHnyJAAgJycHPXr0wNy5c9GzZ0/ExcXdzS4RERHJ0l0FgXbt2sHFxQWpqakAAIPBgICAAOj1eiQlJaG8vBzFxcVIT0+HTqdDr169sGzZMixevBh79uzB+fPn0blzZ0yYMAEA0KJFC/To0QOxsbHia/zxxx84evQo+vfvj/T0dLz77rv45z//ifj4eAQGBiIiIgKlpdeuB3D58mW0bt0au3btQnBw8L22CRERkWzc9VVvrk8PqNVqJCUlYfny5fD29oZCoUBGRgaKiorg5uYGDw8PhIWFwcvLC7GxsZg7dy7MZjN8fX0xY8YMKJVKAMDQoUMxe/ZshIeHQ6FQ4LvvvsMzzzyDhx56CN9//z0GDBiAzp07AwBeeOEFbNiwAUajEY899hgAoG/fvmjSpMm9t8h9wuMaqsb2kR77QFpsf+nJqQ+qmmK/pyCwefNmHDt2DIIgwMfHBwCg0+mwf/9+WCwW6PV6cf3g4GAEBwejvLwcWVlZWLt2LSZPnozt27fDwcEBarUaTk5OSE5Ohk6nQ1xcHD744AMAQG5uLkwmk9WIgcViQW5urhgEXFxc7nZXagWPa6icyWRi+0iMfSAttr/02Ac33PXpg2q1GkeOHIHBYLD6wg8ICEBKSgoOHToEvV6P/Px8BAQE4NSpU9de0N4eSqUSM2fOxPnz53H27FkAgJ2dHQYPHowdO3Zg3759cHFxwVNPPQUAcHV1xUsvvYQ9e/aI/3399dfo06eP+Lp2dnZ3uytERESydddBwNnZGZ6entiyZYtVENBqtcjKykJ2djb8/PzQqlUrdOzYEQsWLMCJEycAAAUFBVi7di28vb3RunVr8bmhoaH45ZdfsG3bNgwbNkwsHzBgALZu3YrMzEwIgoCEhAQ899xzyM3NvdvqExEREe7xpkM6nQ7r1q2DRqMRy5o1awZPT084ODjA0dERALB06VJ8/PHHCA8Px/nz5+Hg4ICAgABERUXB3v5GFnFxcYFGo0FycjL+/e9/i+V+fn6YOnUq/vWvfyE3Nxfu7u5YuHAh2rZti5ycnHvZBSIiIlmzEwRBkLoStmS3tPbvPAjw7oNV4dyc9NgH0mL7S499cAMvMUxERCRjDAJEREQyxiBAREQkY7KbyDYGHea8EBER0d84IkBERCRjDAJEREQyxiBAREQkYwwCREREMsYgQEREJGMMAkRERDImu9MHVQm+QELtXGaYlxUmIqL6hiMCREREMsYgQEREJGMMAkRERDImyaS2SqWCg4MD7O2v5RBBENCyZUuMHj0agwcPBgCEhYUhJCQEzz33nNVzZ8yYgfbt22P8+PFiWXl5OWbMmIGuXbvetj4RERFVTrKj29atWwcvLy8AQFlZGXbt2oU5c+bA19cX7dq1q/F2cnNzsXDhQuzbtw9du3atreoSERE1SHViaqBRo0bo27cvmjZtiuzs7Bo/z2KxYNSoUfDy8kKnTp1qsYZEREQNU504381isWDjxo2wWCzo2LGjWB4VFYXVq1dbrVtUVIT27dsDuBYgvvnmG7i6uiIsLMymdSYiImoIJAsCr776KoBrIQAAdDodPvroI7i5uYnrTJkypcJjBK6zt7eHq6urDWpbMyaTSeoq1BtsK+mxD6TF9peenPrA39+/0mWSBYHPPvsMXl5e+Ouvv/DWW2+hefPmePLJJ6Wqzn1RVUPTDSaTiW0lMfaBtNj+0mMf3CD5MQKPPvooPvjgA+zevRv/+c9/pK4OERGRrEgeBACgdevWmDZtGtasWYOsrCypq0NERCQbdSIIAEBoaCj8/f3x7rvvoqysTOrqEBERyYIkxwgYjcYKy6Ojo8V/f/LJJxWus3jx4grLK1ufiIiIKldnRgSIiIjI9hgEiIiIZIxBgIiISMbqxJUFbckYdJjnjhIREf2NIwJEREQyxiBAREQkYwwCREREMsYgQEREJGMMAkRERDLGIEBERCRjsjt9UJXgCySU3pdtCW/KrvmIiKiB4YgAERGRjDEIEBERyRiDABERkYwxCBAREcmY5EFg9uzZ0Gq1OHPmjFi2fft2qNVqBAYGIjAwEN26dcPzzz+Pbdu2WT33119/xejRo9GjRw8MGjQImzdvtnHtiYiI6jdJD3u/dOkS9u3bh6effhqbN2/GhAkTxGVKpRLr168HAJSXl+PgwYOYNWsWSktLMXz4cFy6dAnTpk3DW2+9hd69e+P333/H66+/Dg8PD2g0Gql2iYiIqF6RdERgx44d6NKlC0aMGIGtW7fCYrFUuJ69vT00Gg0iIiLw8ccfo7y8HKdPn0ZAQAD69u0Le3t7dOjQAf7+/khNTbXxXhAREdVfkgaBrVu3YuDAgfD19cXDDz+M+Pj4KtfX6XQoKCjAyZMnoVQq8d5774nLLl26hF9//RXe3t61XW0iIqIGQ7KpgcOHD8NsNqNbt24AgGHDhuHbb79F3759K32Os7MzAMBsNluVm81mTJ06FY8//ji6d+9ee5W+hclkstlrNTRsO+mxD6TF9peenPrA39+/0mWSBYGtW7fiwoUL6NevHwCgtLQUFy9exJEjRyp9zoULFwAA7u7uYtlff/2FqVOn4tFHH8XChQthb2+7QY6qGpYqZzKZ2HYSYx9Ii+0vPfbBDZIEAbPZjPj4eKxatQoeHh5i+QcffIBvvvmm0s5JSkqCq6srXF1dAQCZmZmYPHky+vbti4iICJuGACIiooZAkiCwY8cOtGnTBp07d7YqHzRoEKZNm4b27dtblZeVleGXX35BdHQ0Xn/9ddjZ2eHcuXOYPHkyRo0ahTFjxtiu8kRERA2IJEFg27Zt6N27923larUazZs3R2lpKY4ePYrAwEAAQOPGjeHh4YFp06aJz/vuu+9QUFCAzz77DJ999pm4jZEjR2LSpEm22REiIqJ6TpIg8PXXX1dYbm9vj7i4OADAK6+8UuU2xo4di7Fjx973uhEREckJJ9WJiIhkjEGAiIhIxhgEiIiIZEzSew1IwRh0mOeOEhER/Y0jAkRERDLGIEBERCRjDAJEREQyxiBAREQkYwwCREREMsYgQEREJGOyO31QleALJJTel20Jb8qu+YiIqIHhiAAREZGMMQgQERHJGIMAERGRjNWbSe4pU6YgJSUFAFBSUgI7Ozs0btwYANC3b18kJydjxowZCAwMlLKaRERE9Uq9CQJRUVHiv2fMmIH27dtj/PjxYlloaKgU1SIiIqrXODVAREQkYwwCREREMsYgQEREJGP15hiBushkMkldhXqLbSc99oG02P7Sk1Mf+Pv7V7qMQeAeVNWwVDmTycS2kxj7QFpsf+mxD27g1AAREZGMMQgQERHJWL2cGli8ePFtZdu3b5egJkRERPUbRwSIiIhkjEGAiIhIxhgEiIiIZKxeHiNwL4xBh3nKCBER0d84IkBERCRjDAJEREQyxiBAREQkYwwCREREMsYgQEREJGMMAkRERDLGIEBERCRjsruOgCrBF0gorfH6wpuyayIiIpIRjggQERHJGIMAERGRjDEIEBERydgdBYHw8HBERkZalU2aNAkajQaFhYViWUpKCgIDAzF27FjodDoEBgYiMDAQQUFBiIiIQHZ2trju9u3b8dJLL932WomJiQgNDb3T/SEiIqI7cEdBQKvVIiUlRXx89epVpKWlwcvLC8nJyWK50WiESqWCQqFAREQEEhMTkZiYiNjYWCiVSoSFhSEvL+/+7QURERHdlTsOApmZmSgqKgIAHDhwAEqlEiEhITAYDOJ6RqMRer3+tuc3bdoUEydOhJeXF2JiYu6oojt37sSIESPQo0cPjB07Funp6QCAnJwc9OjRA3PnzkXPnj0RFxd3R9slIiKSszs6N65du3ZwcXFBamoq1Go1DAYDAgICoNVqERMTg/LyclgsFqSnp2P27Nn46aefKtyOTqdDQkKC+Pj3339Hz549rdYpKytD8+bNAQDJyclYuHAhli9fjk6dOmHHjh144403sGnTJgDA5cuX0bp1a+zatQvl5eV3sktERESydscnyV+fHlCr1UhKSsLy5cvh7e0NhUKBjIwMFBUVwc3NDR4eHpVuw9nZGWazWXzs4+OD9evXW62TmJiIxYsXAwDi4uLQv39/+Pn5AQAGDRqEbdu2Yc+ePeLIQ9++fdGkSZM73Z1qmUym+75NYrvWBewDabH9pSenPvD396902V0Fgc2bN+PYsWMQBAE+Pj4Arv3K379/PywWS4XTAje7cOEC3N3da/yaBQUF4utc5+7ujvz8fPGxi4vLHexFzVXVeHR3TCYT21Vi7ANpsf2lxz644Y5PH1Sr1Thy5AgMBoPVF35AQABSUlJw6NChaoNAcnIyHn/88Rq/pru7O3JycqzKcnJy0KJFC/GxnZ1djbdHRERE19xxEHB2doanpye2bNli9YWv1WqRlZWF7OxscQj/VmazGdHR0Th58iRGjhxZ49fs378/4uLicOjQIZSWluK7777D8ePHbzuugIiIiO7MXV1IX6fTYd26ddBoNGJZs2bN4OnpCQcHBzg6OorlK1aswMqVK2FnZwcnJyd06dIFa9asgaura41fr0uXLnjnnXewcOFC5Obmol27doiKiqpwpICIiIhqzk4QBEHqStiS3dKa33AI4E2HagPn5qTHPpAW21967IMbeIlhIiIiGWMQICIikjEGASIiIhmT3QS4Megw54WIiIj+xhEBIiIiGWMQICIikjEGASIiIhljECAiIpIxBgEiIiIZYxAgIiKSMdmdPqhK8AUSanaZYV5emIiIGjqOCBAREckYgwAREZGMMQgQERHJGIMAERGRjN1xEAgPD0dkZKRV2aRJk6DRaFBYWCiWpaSkIDAwEGPHjoVOp0NgYCACAwMRFBSEiIgIZGdnAwBOnz4NtVqNzMzM217r4MGD6NGjB65cuXKn1SQiIqIauOMgoNVqkZKSIj6+evUq0tLS4OXlheTkZLHcaDRCpVJBoVAgIiICiYmJSExMRGxsLJRKJcLCwpCXl4fWrVtDq9UiNjb2ttf6/vvv0adPHzg5Od3l7hEREVFV7ioIZGZmoqioCABw4MABKJVKhISEwGAwiOsZjUbo9frbnt+0aVNMnDgRXl5eiImJAQAMGTIEO3fuRGnpjdP6zGYzdu/ejWHDhqGsrAxr1qxBaGgoevXqhXnz5sFsNgMAtm/fjtdeew2jR49GSEgITp06dae7REREJFt3HATatWsHFxcXpKamAgAMBgMCAgKg1+uRlJSE8vJyFBcXIz09HTqdrtLt6HQ6/PrrrwCAwMBAKBQKqyCxc+dO+Pj4wMfHB1999RUSEhKwZs0abNu2DUVFRViyZIm47uHDhzFp0iR89913aNOmzZ3uEhERkWzd1RVzrk8PqNVqJCUlYfny5fD29oZCoUBGRgaKiorg5uYGDw+PSrfh7Ows/qpXKBQYOHAgduzYgZ49ewK4Ni3w7LPPAgC+++47vPHGG3B3dwcATJkyBYMGDcI///lPAICrqyvUavXd7EqVTCbTfd8mXcO2lR77QFpsf+nJqQ/8/f0rXXbXQWDz5s04duwYBEGAj48PgGu/8vfv3w+LxVLhtMDNLly4IH6xA8DgwYMxYsQIXLx4EWfOnMGff/6Jp59+GgCQm5uLOXPmYN68eTcqrlAgNzcXAODi4nI3u1GtqhqO7p7JZGLbSox9IC22v/TYBzfcVRBQq9WYP38+DAaD1Rd+QEAAtm3bhpKSEowZM6bKbSQnJ6NTp07i40ceeQR+fn7YtWsXTp06hf79+8PR0RHAtV/8//d//4euXbsCAEpLS/Hnn3/Cw8MDqampsLOzu5vdICIikr27uo6As7MzPD09sWXLFqsgoNVqkZWVhezsbPj5+VX4XLPZjOjoaJw8eRIjR460WjZkyBDs2rUL8fHxGDZsmFg+YMAArFmzBmfPnkVpaSlWrVqFKVOmQBCEu6k+ERER/e2u76qj0+mwbt06aDQasaxZs2bw9PSEg4OD+GseAFasWIGVK1fCzs4OTk5O6NKlC9asWQNXV1erbXbv3h2LFy+Gp6cn2rZtK5a/8sorsFgsGDNmDAoLC9GhQwesWLECCgVvCkRERHQv7ASZ/ay2W1qzOw8CvPtgbeHcnPTYB9Ji+0uPfXADLzFMREQkYwwCREREMia7sW9j0GEOBxEREf2NIwJEREQyxiBAREQkYwwCREREMsYgQEREJGMMAkRERDLGIEBERCRjDAJEREQyJrvrCKgSfIGEml1mmJcYJiKiho4jAkRERDLGIEBERCRjDAJEREQyZpNJ8PDwcPzjH/9AeHi4WDZp0iQYjUbEx8fjwQcfBACkpKRgypQp8Pb2xpEjR6BQXKueQqGAr68vJk+ejPbt2wMASkpKsGzZMsTHx8NiscDf3x8zZ85Eq1atbLFLREREDYJNRgS0Wi1SUlLEx1evXkVaWhq8vLyQnJwslhuNRqhUKigUCkRERCAxMRGJiYmIjY2FUqlEWFgY8vLyAACffvopjh8/js2bNyM+Ph7Ozs5YsmSJLXaHiIiowbBZEMjMzERRUREA4MCBA1AqlQgJCYHBYBDXMxqN0Ov1tz2/adOmmDhxIry8vBATEwMAGD9+PKKiouDs7Ixz587h8uXLaN68uS12h4iIqMGwSRBo164dXFxckJqaCgAwGAwICAiAXq9HUlISysvLUVxcjPT0dOh0ukq3o9Pp8OuvvwIAGjVqBEdHR3z88ccIDQ1Feno6Ro8ebYvdISIiajBsdqL89ekBtVqNpKQkLF++HN7e3lAoFMjIyEBRURHc3Nzg4eFR6TacnZ1hNputysaMGYPRo0dj5cqVmDx5MjZu3CgeW3CvTCbTfdkO3Y5tKz32gbTY/tKTUx/4+/tXusymQWDz5s04duwYBEGAj48PgGu/8vfv3w+LxVLhtMDNLly4AHd3d6syBwcHANcOSNy0aROOHTuGDh063Jc6V9VwdPdMJhPbVmLsA2mx/aXHPrjBZqcPqtVqHDlyBAaDweoLPyAgACkpKTh06FC1QSA5ORmPP/44AGDevHnYtGmTuKysrAyCIKBZs2a1swNEREQNkM2CgLOzMzw9PbFlyxarL3ytVousrCxkZ2fDz8+vwueazWZER0fj5MmTGDlyJADgySefxPr165GTk4OioiIsXboUnTt3rnJqgYiIiKzZ9GL6Op0O69atg0ajEcuaNWsGT09PODg4wNHRUSxfsWIFVq5cCTs7Ozg5OaFLly5Ys2YNXF1dAQDDhg1DQUEBXn31VVgsFmi1Wrz//vu23B0iIqJ6z04QBEHqStiS3dKa3XAI4E2Hagvn5qTHPpAW21967IMbeIlhIiIiGWMQICIikjEGASIiIhmT3SS4Megw54WIiIj+xhEBIiIiGWMQICIikjEGASIiIhljECAiIpIxBgEiIiIZYxAgIiKSMdmdPqhK8AUSKr/MMC8rTEREcsIRASIiIhljECAiIpIxBgEiIiIZq3NBoKioCOfOnZO6GkRERLJQ54LAuHHjkJGRccfPCwkJgdForIUaERERNVx1LghcuHBB6ioQERHJRp06V+7NN99Ebm4uZs6cicmTJ0MQBGzYsAGXLl3CE088gbfeegtt27YFAOzcuROrV6/GhQsXMGzYMGkrTkREVE/VqRGBpUuXwt3dHYsWLUKTJk2wfv16LF26FLt27YKvry/Cw8NRVFSErKwsvPfee5g9ezbi4+NhZ2eHixcvSl19IiKieqdOjQjcLC4uDi+88AK8vb0BAK+99hq2bt2KQ4cOITU1FXq9HiqVCgAwYcIEfPvtt/fldU0m033ZDlWN7Sw99oG02P7Sk1Mf+Pv7V7qszgaB8+fPw93dXXxsb28PNzc35Ofn49y5c2jZsqW4rHHjxnB1db0vr1tVY9H9YTKZ2M4SYx9Ii+0vPfbBDXVqauBm7u7uOH36tPi4vLwcubm5aNGiBVxdXa2WlZaW4vz581JUk4iIqF6rc0GgcePGuHz5MgYMGICvv/4ax44dg8ViwaeffgoA6Nq1K3r37o0DBw4gMTERpaWl+PTTT3H58mWJa05ERFT/1LkgMGDAAMyfPx85OTkYNWoUpk+fjpCQEBw6dAjR0dF44IEH0LZtW/z73//G8uXLERQUhDNnzqBNmzZSV52IiKjeqXPHCIwdOxZjx44VH48aNarC9Xr27ImePXvaqFZEREQNU50bESAiIiLbYRAgIiKSMQYBIiIiGatzxwjUNmPQYZ47SkRE9DeOCBAREckYgwAREZGMMQgQERHJGIMAERGRjDEIEBERyRiDABERkYzJ7vRBVYIvkFBa6XLhTdk1CRERyRhHBIiIiGSMQYCIiEjGGASIiIhk7J6CQHh4OCIjI63KJk2aBI1Gg8LCQrEsJSUFarUaI0aMqHA7//rXvzBv3rx7qQoRERHdhXsKAlqtFikpKeLjq1evIi0tDV5eXkhOThbLjUYjVCoV/ve//yEzM9NqG2azGbt378awYcPupSpERER0F+45CGRmZqKoqAgAcODAASiVSoSEhMBgMIjrGY1GBAUFISAgADt27LDaxq5du/DYY4/hqaeeQm5uLqZOnYqQkBAMGTIE33//vbheUVERlixZgr59+6JPnz5YsWIFLBYLAODjjz9GREQERowYgX79+sFsNt/LbhEREcnGPQWBdu3awcXFBampqQAAg8GAgIAA6PV6JCUloby8HMXFxUhPT4dOp8PQoUOxc+dOlJbeOH3vu+++w7Bhw1BWVoapU6eiffv22LlzJ95//32sWrUKRqMRABAZGYk//vgDX3/9Nb7++mtkZGTgP//5j7idgwcPYuHChfj222/RrFmze9ktIiIi2bjngwVvnh5ISkqCXq+HUqmEQqFARkYG0tLS4ObmBg8PD+j1ejg4OCApKQkAcPz4cfzxxx/o27cvMjIykJubi9dffx2NGzeGj48Phg4diq1bt0IQBHz//feYPHkymjdvjocffhjjx4/H1q1bxXoolUp4eXkxBBAREd2Be756jlarxebNm3Hs2DEIggAfHx8AgE6nw/79+2GxWKDX6wEA9vb2GDRoEGJjY9G9e3d899136NOnD5ycnJCbm4vLly8jODhY3HZ5eTk6dOiAgoICFBcXY/z48bCzswMACIKA0tJSFBcXAwBcXFzudVcAACaT6b5sh6rGdpYe+0BabH/pyakP/P39K112z0FArVZj/vz5MBgM4hc+AAQEBGDbtm0oKSnBmDFjxPJBgwZh6NChKCgoQFxcHKKjowEArq6uaNmypdUxBOfOnYMgCHB2dkbjxo3x1VdfwcPDA8C1AxPPnTsHBwcHABADwr2qqrHo/jCZTGxnibEPpMX2lx774IZ7nhpwdnaGp6cntmzZYhUEtFotsrKykJ2dDT8/P7G8VatW6Nq1K5YsWQIPDw9xBKFjx45wdHTEF198gdLSUuTl5eH111/Hxo0b0ahRI/Tp0wcrV65EYWEhrl69igULFmDu3Ln3Wn0iIiJZuy8XFNLpdMjPz4dGoxHLmjVrBk9PTzzxxBNwdHS0Wn/YsGHYtWuX1SmDCoUCkZGRMJlM6N27N1566SV07doV48aNAwC8+eabaN68OZ599lnxzICFCxfej+oTERHJlp0gCILUlbAlu6WV33AI4E2HbIFDctJjH0iL7S899sENvMQwERGRjDEIEBERyRiDABERkYzJbkLcGHSY80JERER/44gAERGRjDEIEBERyRiDABERkYwxCBAREckYgwAREZGMMQgQERHJGIMAERGRjMnuOgKqBF8goeL7DfA+A0REJDccESAiIpIxBgEiIiIZYxAgIiKSMZsEgfDwcERGRlqVTZo0CRqNBoWFhWJZSkoKAgMDMXbsWOh0OgQGBiIwMBBBQUGIiIhAdnZ2hdv/7rvvEBISUqv7QERE1BDZJAhotVqkpKSIj69evYq0tDR4eXkhOTlZLDcajVCpVFAoFIiIiEBiYiISExMRGxsLpVKJsLAw5OXlWW37zz//xPLly22xG0RERA2OzYJAZmYmioqKAAAHDhyAUqlESEgIDAaDuJ7RaIRer7/t+U2bNsXEiRPh5eWFmJgYsbysrAxz5szBkCFDan8niIiIGiCbBIF27drBxcUFqampAACDwYCAgADo9XokJSWhvLwcxcXFSE9Ph06nq3Q7Op0Ov/76q/h47dq1+Mc//oGAgIDa3gUiIqIGyWYnzl+fHlCr1UhKSsLy5cvh7e0NhUKBjIwMFBUVwc3NDR4eHpVuw9nZGWazGQBw5MgRxMXFYf369cjIyLgvdTSZTPdlO1Q9trX02AfSYvtLT0594O/vX+kymwaBzZs349ixYxAEAT4+PgCu/crfv38/LBZLhdMCN7tw4QLc3d1RVFSEOXPmYPbs2XBycrpvdayqoej+MZlMbGuJsQ+kxfaXHvvgBpudPqhWq3HkyBEYDAarL/yAgACkpKTg0KFD1QaB5ORkPP744zhy5Aj++usvREREoGfPnpg6dSouXbqEnj17Ijc3t7Z3hYiIqMGw2YiAs7MzPD09sWXLFkRERIjlWq0WS5YsQWlpKfz8/Cp8rtlsxrp163Dy5EksWLAArq6u2Ldvn7jcaDTi7bffxs8//1zbu0FERNSg2PTi+jqdDuvWrYNGoxHLmjVrBk9PTzg4OMDR0VEsX7FiBVauXAk7Ozs4OTmhS5cuWLNmDVxdXW1ZZSIiogbNThAEQepK2JLd0opvOATwpkO2wrk56bEPpMX2lx774AZeYpiIiEjGGASIiIhkjEGAiIhIxmQ3KW4MOsx5ISIior9xRICIiEjGGASIiIhkjEGAiIhIxhgEiIiIZIxBgIiISMYYBIiIiGRMdqcPqhJ8gYQblxnmZYWJiEjOOCJAREQkYwwCREREMsYgQEREJGMMAkRERDImeRCYPXs2tFotzpw5I5Zt374darUagYGBCAwMRLdu3fD8889j27ZtFW4jPT0dffr0sVGNiYiIGg5JD5m/dOkS9u3bh6effhqbN2/GhAkTxGVKpRLr168HAJSXl+PgwYOYNWsWSktLMXz4cACAIAj4/vvvsXz5cjRq1EiSfSAiIqrPJB0R2LFjB7p06YIRI0Zg69atsFgsFa5nb28PjUaDiIgIfPzxxygvLwcA/Oc//8GGDRswduxYW1abiIiowZA0CGzduhUDBw6Er68vHn74YcTHx1e5vk6nQ0FBAU6ePAkAGDRoEGJiYvDEE0/YorpEREQNjmRTA4cPH4bZbEa3bt0AAMOGDcO3336Lvn37VvocZ2dnAIDZbAYAuLq63nM9TCbTPW+D7hzbXXrsA2mx/aUnpz7w9/evdJlkQWDr1q24cOEC+vXrBwAoLS3FxYsXceTIkUqfc+HCBQCAu7v7fatHVY1DtcNkMrHdJcY+kBbbX3rsgxskCQJmsxnx8fFYtWoVPDw8xPIPPvgA33zzTaWdk5SUBFdX1/syEkBEREQSBYEdO3agTZs26Ny5s1X5oEGDMG3aNLRv396qvKysDL/88guio6Px+uuvw87Ozib1tFtaWv1K94D3OSAiIqlJ8k20bds29O7d+7ZytVqN5s2bo7S0FEePHkVgYCAAoHHjxvDw8MC0adMqfJ4clJWV4YsvvsD27dtRVlYGi8WCoKAghIeHo0mTJpg5cya8vb3x6quv1lod/vjjD8yaNQsFBQVwcnLC+++/f1toIyKi+kWSIPD1119XWG5vb4+4uDgAwCuvvFLj7alUKvz888/3pW511dy5c3Hx4kWsW7cODz74IK5cuYI333wTs2bNwpIlS2xShzfffBOjR49GaGgo9u7di/DwcGzfvt1mIzRERHT/cWy6Hvjzzz+xfft2GAwGNGvWDADg5OSEefPm4dChQ7etv2nTJnzzzTewWCy4ePEixo0bhxdeeAFnzpzB22+/jYKCAgBAjx49EBERUWn5zfLy8nD8+HH0799fXGfevHnIyMjAk08+WYt7T0REtUnySwxT9X777Td4eXmJIeC6li1b3jZVcvnyZWzcuBGffPIJtm3bhuXLl4sjBt9++y08PDywdetWfPXVVzh58iQKCwsrLb/Z6dOn0apVK9jb33jLuLm5ITc3t5b2moiIbEF2IwLGoMP17pQRe3t78WqK1WnatCk++ugj7N27F3/88QcyMzNx5coVAEBgYCDCwsJw+vRp6PV6TJ8+HQ8++GCl5TcrLy+/bQpAEARe2pmIqJ7jiEA90KlTJxw/fly8kNJ1eXl5CAsLQ1FRkViWm5uLwYMH46+//oK/v7/VEH+nTp3w888/47nnnsNff/2FESNGID09vdLymz3yyCM4c+YMBEEQy/Lz8+/rNR2IiMj2ZDciUB+5ubkhNDQU//znP7FgwQI0a9YMZrMZc+fORfPmzeHo6Cium56ejhYtWuD1118HAHz00UcArp11sHz5cgiCgLfeegshISE4evQosrKysHPnzgrLn3rqKXG77u7ueOyxxxAXF4f+/fsjMTER9vb28PHxsW1jEBHRfcUgUE/MmTMHq1atwsiRI9GoUSOUlJTg6aefxuTJk63WCwgIwKZNm9CnTx/Y2dlBrVajRYsWOHnyJEaPHo2ZM2diwIABaNKkCZRKJfr374+LFy9WWH6rZcuWYfbs2Vi9ejWaNGmCyMhIq2MGiIio/rETbh7rlQFeVlJ67APpsQ+kxfaXHvvgBv6cIyIikjEGASIiIhljECAiIpIxBgEiIiIZYxAgIiKSMQYBIiIiGWMQICIikjEGASIiIhljECAiIpIxBgEiIiIZk9W9Bq5fTbm4uFjimhD7QHrsA2mx/aUntz5o0qTJbbeTB2R2r4Hi4uLbbq9LREQkB0899RQcHBxuK5dVEBAEASUlJVJXg4iIyOY4IkBERES34cGCREREMsYgQEREJGMMAkRERDLGIEBERCRjDAJEREQy1mCDwM6dOzFixAgMGTIE33777W3Ljx49ipdeeglDhw7Fe++9h9LSUglq2bBV1wd79uzBCy+8gOeffx7Tp0/HpUuXJKhlw1ZdH1xnMBgwcOBAG9ZMHqpr/z/++ANhYWF4/vnn8cYbb/BvoBZU1weZmZl4+eWX8fzzzyMiIgKFhYUS1FJiQgOUl5cnhIaGChcuXBCuXLkijBw5UsjOzrZaZ8SIEUJqaqogCIIwb948YePGjVJUtcGqrg8KCwuF3r17C3l5eYIgCMLq1auFJUuWSFXdBqkmfweCIAhnz54Vhg0bJgwYMECCWjZc1bV/eXm5MGTIEGHfvn2CIAhCVFSUEBkZKVV1G6Sa/A28+uqrgsFgEARBEJYtWyZER0dLUVVJNcgRgQMHDkClUsHZ2RkPPPAAQkJC8PPPP4vLT58+jeLiYnTs2BEAEBoaivj4eKmq2yBV1welpaV4++230apVKwCAl5cXcnNzpapug1RdH1w3f/58jBs3ToIaNmzVtX9mZiYeeOAB6PV6AMArr7yCZ599VqrqNkg1+RsoLy/H5cuXAQBFRUUVXnmvoWuQQeDMmTNwdXUVH7u6uiI/P7/Gy+neVdfGzZs3R1BQEIBrf3zr1q1Dz549bV3NBq0m7/MNGzagQ4cOYiim+6e69j916hRcXFzw7rvvYtSoUVi0aBEeeOABKaraYNXkb2Dq1Kn497//jd69e2P//v0YNmyYraspuQYZBMrLy60uoygIgtXj6pbTvatpG5vNZkRERMDb2xsDBgywZRUbvOr64NixY9i9ezdeffVVKarX4FXX/mVlZTCZTBg+fDi++uorPProo1i+fLkUVW2wquuDoqIivPfee4iOjsaPP/6I4cOHY86cOVJUVVINMgi4ubnh7Nmz4uNz586hZcuWNV5O964mbXz27Fm89tpr8Pb2xuzZs21dxQavuj74+eefcfbsWbz88ssIDw/HmTNn8Nprr0lR1QapuvZ3cXHBY489hieeeAIA0Lt3b/z22282r2dDVl0fZGdnw8HBAU899RQAYNiwYTCZTDavp9QaZBBQq9U4ePAgCgoKUFRUhN27d0On04nLW7dujSZNmuDXX38FAMTFxYnzdHR/VNcHZWVlmDp1Kp5++mlMnz6dIzK1oLo+GD9+PLZs2YKYmBhERkaiZcuW+PTTTyWsccNSXft36tQJBQUF+P333wEA//3vf9GhQwepqtsgVdcHbdq0QV5eHv744w8AwN69e8VgJicKqStQG1q1aoXXX38d48ePR2lpKQYNGoSnnnoKU6ZMwYQJE/DEE09g/vz5mD9/Pi5fvowOHTpg5MiRUle7QamuD/Ly8pCZmYmysjLs3r0bAPD4449zZOA+qsnfAdWemrT/0qVLMX/+fBQVFaFVq1Z49913pa52g1KTPpgzZw7eeecdCIKAFi1ayHJqgHcfJCIikrEGOTVARERENcMgQEREJGMMAkRERDLGIEBERCRjDAJEVO+dOnVK6irUW/er7epTH9SnutoCg4DMTZw4EUePHgUABAcH488//wRw7V4Ay5YtQ3BwMDp37ozAwED861//wsWLF8XnKpVK8Rzom2k0Guzfv9+qbOPGjVAqlfjhhx+syv/8808olUp06dJF/K9r16544403kJeXd9/2c8uWLRg6dOg9b+fDDz/Ehx9+CAAwGo145513qn3OzW3cUHz00Ud46623pK4GAODLL7/EkiVLpK7GXbl8+TKUSqX4d1eVl156CV9++eV9ff371XYZGRl4/vnn70ONasfN79eff/4ZU6dOvavtJCQkIDg4uNr1SktL8eKLL+L8+fN39Tq2xiAgY7GxsWjevDmUSuVty1atWoX9+/dj/fr1+PXXX7Fp0yacPn0ab7/99l291rfffovhw4dX+kFmMBiQkpKClJQU/Pe//0WTJk0wZcqUu3otW1GpVCgsLMS+ffsqXaeqNq7PJkyYUGe+fAsKCqSuQr11v9qusLAQFovlvmyrNtz8fr148SLKy8tr9fUUCgXGjBmDBQsW1Orr3C8MAhL4888/odFo8Pnnn0On00Gj0WDjxo34+OOPodVqERAQgO3bt4vrHzx4EMOGDYNKpcKIESOQmpoqLktOTsbIkSOh1Wrh5+eHKVOm4OrVqwCu/YJYvnw5Bg0aBD8/P7z44oviLw9BELBq1apKU3xaWhr0ej0effRRANcu1fnOO+/Azc3tjvc3MzMT//vf//DOO+/g6NGjyMzMrHL9Bx54AAMHDqxwtGH69Ol4//33xcdXrlxB586dkZ2djYKCAkyfPh3BwcHw9fVFaGhohZcLvXV04NZfZUePHsVLL70ElUqF0NBQ7N27t9K6Pvvss4iOjq5wWUVt/MUXXyA0NBT+/v7Q6/Xi6MKyZcusgo8gCAgODsZ///tfAEBMTAyeeeYZaDQaTJo0CWfOnAEA7N+/H3379sW4ceOgVquxf/9+ZGRkYMyYMejWrRt8fX0xduxY8TKrZrMZU6dOhb+/P/r164eVK1da/cLZtWsXBgwYAJVKhdGjR+PEiRMV7tuHH34o1nfmzJlYvHgxRo4cic6dO+PFF19EamoqRo4ciS5dumDs2LEwm80Arr0nFy9ejN69e6NLly6YPHkyLly4AODadd/nzp2LXr16oXPnznjmmWes7gr6448/on///ujSpQuGDx+O9PR0/Pjjj/j4448RHx+P4cOHV1jXdevWISQkBF27dsXYsWNx/Phxse1CQ0OxcOFCqNVqdO/eHWvWrKlwG1u2bMGECRMwc+ZMdOnSBc888wwOHjyI6dOno0uXLujfv7/4vi4tLcWKFSvQvXt3aDQaTJkyxWp0a+3atejWrRs0Gg3Wrl1r9To5OTmYMGECNBoNnnnmGWzevLnC+txq3759GDp0KPz8/DBo0CCr9+ytI3dTpkzBhx9+WGHbKZVKfPLJJ9Dr9dBoNFi2bJn4pXnriMSXX36Jl156CefOncO4ceNw4cIFdOnS5bZwcaefd5X9jQDXRuEGDhwIlUqFSZMmYdKkSeLyqj7vrr9fU1NTMWfOHBw5cgQBAQEAro2EJiQkiK/x/vvvY+bMmQCA4uJi/N///R/8/f0RHBx820hnVZ/NwcHBOHjwYKV/Q3WKNHc/lrdTp04JPj4+wnvvvSeUlJQI33zzjfD4448LCxYsEEpKSoSvvvpKUKvVgiAIwl9//SV06dJF+OmnnwSLxSLExcUJarVaKCgoEC5fviz4+fkJ8fHxgiAIwunTp4WgoCDh22+/FQRBEF588UUhJCRE+N///idcunRJeOGFF4TZs2cLgiAIRqNR6NGjh1W9goKChFOnTgmCIAibN28WnnrqKWHmzJlCbGyscPr06dv2w8fHR+jSpYvg7+9v9Z9SqRR++eUXcb25c+cKCxYsEARBEN59911h1qxZt7WF2WwWy/Ly8oTx48cL48ePv+019+7dK/Ts2VMoLy8XBEEQtm3bJgwdOlQQBEF45513hGnTpglXr14ViouLhTlz5gjPP/+8uD9Dhgy57d+CIAhms1nw8fERTp06JRQWFgoBAQHCl19+KVgsFuGXX34RVCqVcPz4cUEQrt0zPioqSnyuxWKxWn6zW9v44MGDgk6nE06cOCE+ViqVwh9//CEcO3ZM6NSpk9gOBw8eFPR6vVBaWirExcUJPXr0EH7//XehqKhIWLhwoTBq1ChBEAThl19+EXx8fIRNmzYJV65cESwWi/D0008LX3zxhVBeXi6cP39eGD58uLB8+XJBEAThrbfeEl577TXh0qVLwsmTJ4VevXoJQUFBgiAIwuHDhwV/f3/BaDQKJSUlwueffy706tVLKCkpuW3foqKihMmTJwuCIAhvv/22oNFohKysLMFsNgu9e/cWAgIChGPHjgkXLlwQnnnmGeHLL78UBOHae1Kv1wtHjhwRCgsLhbCwMCEiIkIQBEFYuXKl8OKLLwqXLl0SSktLhdWrVwvdu3cXBEEQfv/9d6Fjx47C3r17hbKyMuHLL78UevToIZSWllrV5VYbNmwQAgMDhSNHjgjFxcXChx9+KAQHBwtXr14V2y46OlqwWCzCrl27hA4dOlT4Xt+8ebPg4+Mj7NixQygrKxPefPNN4YknnhB27twpFBcXC9OnTxfr8MEHHwgDBgwQTp06JVy5ckWYNWuW8Nxzzwnl5eVCQkKCoNFohCNHjghXrlwRpk+fLr73SktLhdDQUGHp0qVCcXGxcOTIESEgIEBITk4W2279+vW31e162/z444+CxWIR9uzZI/j6+gqZmZmCIFz7Oz169Ki4/uTJk8X38K1t5+PjIzz//PPCuXPnhJMnTwpBQUFCTExMha+/fv164cUXXxTfh9c/s251J593Vf2NFBQUCCqVSvj2228Fi8UibN26VfDx8RH3parPu5v389a//6CgIGH37t3i40WLFglvv/22+O/nnntOOHfunHD69GlhwIAB4t9LVZ/N182bN0/44IMPKmyXuoQjAhJ65ZVX0LhxY2i1WpSVlYmPAwMDceHCBVy9ehWxsbHQaDR4+umnoVAo0LdvX/j4+ODHH3+Eg4MDtm7dipCQEBQWFiI/Px/Nmze3+vUxcOBAtGnTBg8++CB69eolXlPbaDSiU6dOldZt6NCh+OSTT1BcXIz58+ejR48eGDhwIJKTk63W27BhA4xGo9V/zs7O4vKioiLExsaK91kfOXIkYmNjrY41AIAePXpApVLB398fw4YNQ9OmTTF//vzb6hUQEACLxYJDhw4BuDb0PmjQIADXbic6b948NGrUCDk5OXjooYfu+DiDvXv3okWLFhg1ahQUCoXY9lu3bq1wfYVCgQ4dOuDgwYO3Lbu1jZ988kls2bIFbdu2xdmzZ2GxWODo6Ij8/Hy0b98e3t7e4r3SY2NjMWDAADRq1AibNm3CmDFj4O3tDQcHB0ybNg2HDx8Wf2nY2dkhNDQUDzzwABQKBT777DOMGjUKV69eRV5eHh5++GHk5eWhpKQEO3fuxLRp0/Dggw/isccew9ixY8X6bdq0CYMHD4a/vz8aN26MMWPGoLS09LZfQRUJCgqCl5cXmjZtio4dO6JHjx5o3749nJ2d4evri7/++ktc98UXX0SHDh3QrFkzRERE4KeffkJJSQlGjRqFqKgoODk54fTp02jatKnYfz/88AMCAwPRvXt32Nvb4/nnn8fy5cshVHNh1O+++w5jxoxBhw4d0KRJE7z++usoKSnBgQMHAACNGjXCuHHjoFAo0KtXLzg5OVV6INmjjz6Kfv36wd7eHmq1Go888gh69+6NJk2aQKvVIicnR3zNN954Ax4eHnjggQfwz3/+E2lpaTh+/Dji4uIwaNAgdOjQAQ888IDVcRZpaWk4ffo0pk6diiZNmoiXPt+4cWOV+7hjxw7o9Xo888wzUCgU6NGjB4KDg61+Zd+J6dOno0WLFnjsscfw8ssvY8eOHXe1nVvV5POuqr+RPXv24JFHHsGIESOgUCgwePBgdO7c2eo1Kvu8u1s//PADxo0bhxYtWsDd3R3jxo0Tl1X12XzdU089Jb7X6rIGea+B+uL6F6a9/bU89uCDDwKAeAOe8vJy5OTkIDExESqVSnxeaWkp/P390ahRI+zevRvr1q0DcG1Y7+rVq1Yfji1atBD/rVAoxGW5ubnV3nFRp9OJN+jIzs7G119/jfHjxyM+Ph6tWrWq0T7GxcWhsLAQL7/8slhWVFSETZs2Wd3+du/evWjatGm122vUqBFCQ0MRFxeHdu3a4cCBA1i0aBEAID8/H//+97+RnZ2Ndu3aoXnz5tV+UdwqJycH2dnZVu1dVlaGXr16Vfqcli1bIjc397byW9vY3t4eq1atwo8//ggXFxfxjmfXh14HDx6MuLg49OvXDz/++CM+++wzAMDp06exYsUKrFy5UtyWnZ0dcnJyoFAo4OzsjCZNmojLUlNTMW7cOHHK4+LFi2jRogUuXryI4uJiuLu7i+s+8sgj4r9Pnz6N/fv3Y9u2bWKZxWLB6dOnq223m8Nfo0aN8NBDD1nt98394OnpKf7bzc0NFosFFy5cQHFxMebNm4fU1FS0adMGbdq0EZ939uxZq3rb29ujS5cu1dbr3LlzVvtob2+P1q1bIy8vD4899hgefPBBNG7cWFyuUCgqnT9u3ry51T5e/3u9vt3rz7v1NZ2cnMSAfvbsWasbC7m5uUGhuPYxnJOTA7PZDLVaLS4vKyvDk08+WeU+nj9/3ur1gGv9WtF7siZu7h93d3dxGupe1eTzTqFQVPo3kp+fj9atW1tt89b9ruzz7m6dPXvWajr0+lQpgCo/m69r2bLlfT3oubYwCEioJnfca9myJfr164fFixeLZadOncLDDz+MQ4cOITo6Ghs3bkTbtm0BwOoLt7rXruwDr6ysDBqNBlFRUeJdGdu3b49Zs2Zh27ZtOH78eI2DwLfffos333xT/NUOXAsHX3zxBV555ZUabeNWgwYNwmuvvQYvLy9otVq4uLgAAKZNm4bnnnsOX331Fezs7LBt27YKjzOwt7e3OrDp+hw1cK29O3fujK+++kosy83NhYODQ6X1KS0tFT/cbnZrG3/++ef4/fffER8fjwcffBAWiwVxcXHi8n79+uGDDz7ATz/9BBcXF/GmQC1btsTYsWOt5sCzs7PRpk0bpKSkWL1mbm4u3n77bcTExMDX1xcArG6o0qRJE5w+fRoPP/wwAFh9SLVs2RKvvvoqwsPDxbI//vijRseF3MndI/Pz88V/5+TkwNHREc2bN8eECRPQvn17fPTRR1AoFDh48KB4lombmxuOHDkiPk8QBCxZsqTa2yY/8sgjVqMR18P19fdMbbj+mh07dgRw7RiUgoICuLi4oFWrVuLIAXAtNJSWlgK4doMcNzc37NmzR1x+9uzZar/MWrduLd5J9bo///xTDE63vt+rO0AwPz8frq6uAK71z/Uv36r+bmqiJu+Rqv5G3N3drdoOuPZ+/8c//nFH9bhVVft1vb+uB5Jb/14q+2y+rqysrMLPhrqm7tdQ5vr374+EhAQkJydDEASYTCYMHDgQaWlpMJvNsLe3h6OjI8rKyrBt2zYYjUbxg6UqrVu3rjTpN2rUCL169cL777+P1NRUCIKAS5cu4YsvvoCjo6P4AVedrKwspKWlYejQoWjZsqX439ChQ3HmzBmrD7w70aFDB7Ro0QIff/yxVcAwm8144IEHYGdnh+zsbKxZs6bCI5nbtWuHEydO4PDhwyguLsYnn3wifkj17NkTx48fR2xsLMrKypCdnY0RI0ZYHbR2qzNnzlj9Wr3u1jY2m81o3LgxGjdujMuXL+P999+HxWIR+6tFixbQarV4//33MXDgQPF5Q4YMweeff46TJ0+ivLwc69evx7PPPiseFHqzy5cvAwAcHR0hCAL27t2LnTt3wmKxoFGjRhg0aBAiIyNhNpvx119/4fPPP7d6nY0bN+K3336DIAj46aefMGDAgBqNCNyJL7/8EqdOnUJhYSFWrFiB/v37o0mTJjCbzXB0dESjRo1w+vRpREZGArg2KtG3b1/s27cPycnJKC8vR0xMDHbu3CmOhlw/GPFWgwcPxrp163D06FGUlJRg1apVAACtVntf9+nW14yOjsZff/2Fq1evYuHChfDy8oKPjw8GDRqErVu3iu+9pUuXis/z9fWFo6MjPv30U1gsFuTm5uKVV16xCqUV6devH/bv349du3ahrKwMe/fuxe7du9GvXz8AQNu2bREbGwuLxYJ9+/ZZhYaK2i4qKgpmsxknTpzA+vXrMXjwYHE78fHxMJvNOHXqFL7//nur7ZSUlKCkpOSe2q6qv5Hg4GDk5eVh8+bNKC0txc6dO8UpwjvRpEkTXL58WQxYbdu2xQ8//ICioiJkZGSId0MFrk01rFq1Cnl5eThz5ozVwaRVfTZfV9lnQ13DIFDHtW3bFitWrMCSJUvg7++Pt99+G++88w50Oh26deuGPn36IDQ0FHq9Htu3b8eQIUOQnZ1d7XZ1Ot1tvyJuNm/ePISEhOCtt96Cn5+feMTsF198UaMhfAD45ptvoNVqrYbrgGtDgk8//XS1H3BVGTx4MAoLC62OeH/33Xfx2Wefwc/PD2+88QaGDBmCgoKC234B+fr64qWXXsLEiRMRHByMtm3bisOWzZs3x6effoqvv/4aGo0Gr7zyCp5//nmMGDGiwnpYLBYcOXLE6h7n193axq+88goUCgV0Oh169+6NkpIS+Pn5WfXX4MGDkZeXZxUEBg0ahBEjRmDcuHFQqVT47rvv8PHHH1sNx1/Xvn17TJw4EaNHj4Zarcbq1asxcuRI8Uj5GTNmoEmTJggMDERYWBhUKpU4NN61a1fMnDkTM2bMgJ+fHyIjI7FixYp7/sV1q86dO2PixIkICgpCy5YtMWvWLADXRi727NkjHvHdo0cPODk5ITs7G//4xz+wbNkyLFiwACqVCrGxsfjoo4/QqFEj9OzZE7///jt69+5922sNGjQIY8eOxaRJk6DRaHDgwAF8/vnncHJyuq/7dLNx48YhODgYL7zwArp164bz58+LYVOn0+Htt9/GlClTEBAQgFatWonTOo0bN8Ynn3yCAwcOoFu3bhg6dKh4lkhVPD09ER0djdWrV0OlUmHJkiX44IMPxONTZs+eDYPBALVajS+//BIDBgwQn1tR23l4eKB///546aWX8MILL4hBICwsDI0aNUL37t0xZcoUsRy4Ni3p5eUFjUaDkydP3nXbVfU30qxZM0RGRuLTTz+FWq1GXFwcOnbsaDW1UxNdu3YV/19cXIzp06fjzz//hE6nw4IFC6zOKJo0aRJUKhUGDBiAYcOGiSOkQNWfzdcdPny4ws+GOsf2xydSXdG3b18hJSVFfHzzWQNUsVvPGti9e7fwwgsvVLr+rW0stQMHDghXrlwRH3/11VfCc889Z7PXr+zId6obbj3DoC45d+6ckJaWZlU2fPhwYcOGDRLVqGoWi0UIDAwUz4CoyzgiIGOTJk26p1/ldO38/qp+sdW1Nv7oo4+watUqlJWVIT8/H9988w26desmdbWIqlVSUoKXXnoJv/32GwBgz549yMzMrNVpnnuxa9cuaDQa8fituoxBQMb69++PwsLCai/wQxUzGo14+OGHrYYLb1XX2nju3Ln47bffoNFoMGjQIKjVaoSFhUldLaJqubu7491338W0adPQpUsXLF26FMuWLbM6y6GuKC0txfr168ULE9V1doJwj+dXEBERUb3FEQEiIiIZYxAgIiKSMQYBIiIiGWMQICIikjEGASIiIhljECAiIpKx/wdM7XaaqHWYaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x684 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance through SHAP values performed\n"
     ]
    }
   ],
   "source": [
    "shap_values=feature_importance (X_train, X_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d4e3fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=transform_shap (shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5ee9108f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['T2', 'RH2', 'T1', 'RH1', 'PR1', 'AD1', 'PR2', 'AD2', 'Rain', 'WS1',\n",
       "       'WS3', 'WS4', 'WD1', 'WD3', 'WD4', 'WSHor', 'WDHor', 'WSVer', 'WDVer',\n",
       "       'TI', 'WSH', 'WD_bin', 'tod', 'WVeer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d622497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variables</th>\n",
       "      <th>SHAP_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T2</td>\n",
       "      <td>0.149231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RH2</td>\n",
       "      <td>0.097610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1</td>\n",
       "      <td>0.181885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RH1</td>\n",
       "      <td>0.046647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PR1</td>\n",
       "      <td>0.003911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AD1</td>\n",
       "      <td>0.007266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PR2</td>\n",
       "      <td>0.005107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AD2</td>\n",
       "      <td>0.039358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rain</td>\n",
       "      <td>0.000787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WS1</td>\n",
       "      <td>0.692661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WS3</td>\n",
       "      <td>0.004635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WS4</td>\n",
       "      <td>0.726750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WD1</td>\n",
       "      <td>0.080019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WD3</td>\n",
       "      <td>0.021366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>WD4</td>\n",
       "      <td>0.013075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WSHor</td>\n",
       "      <td>0.879718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WDHor</td>\n",
       "      <td>0.024555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WSVer</td>\n",
       "      <td>0.052878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>WDVer</td>\n",
       "      <td>0.022559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TI</td>\n",
       "      <td>0.038749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>WSH</td>\n",
       "      <td>0.107528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>WD_bin</td>\n",
       "      <td>0.059618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tod</td>\n",
       "      <td>0.016510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>WVeer</td>\n",
       "      <td>0.016224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variables  SHAP_abs\n",
       "0         T2  0.149231\n",
       "1        RH2  0.097610\n",
       "2         T1  0.181885\n",
       "3        RH1  0.046647\n",
       "4        PR1  0.003911\n",
       "5        AD1  0.007266\n",
       "6        PR2  0.005107\n",
       "7        AD2  0.039358\n",
       "8       Rain  0.000787\n",
       "9        WS1  0.692661\n",
       "10       WS3  0.004635\n",
       "11       WS4  0.726750\n",
       "12       WD1  0.080019\n",
       "13       WD3  0.021366\n",
       "14       WD4  0.013075\n",
       "15     WSHor  0.879718\n",
       "16     WDHor  0.024555\n",
       "17     WSVer  0.052878\n",
       "18     WDVer  0.022559\n",
       "19        TI  0.038749\n",
       "20       WSH  0.107528\n",
       "21    WD_bin  0.059618\n",
       "22       tod  0.016510\n",
       "23     WVeer  0.016224"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e41ded9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v=np.array(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "813d0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=v.reshape(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "76f854db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.05968182,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        , -0.09837991],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.14081973,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.21122331,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        , -0.20668495, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.19280321,\n",
       "          0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "085246dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "e=pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "18922c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.059682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.089369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.296950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273281</td>\n",
       "      <td>-0.095848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.272807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.126148</td>\n",
       "      <td>-0.048960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.09838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.311346</td>\n",
       "      <td>-0.074599</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182592</td>\n",
       "      <td>-0.166558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.226631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144254</td>\n",
       "      <td>0.227084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.088230</td>\n",
       "      <td>0.062891</td>\n",
       "      <td>0.252543</td>\n",
       "      <td>-0.145355</td>\n",
       "      <td>0.211223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.030068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.047704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.178866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.209635</td>\n",
       "      <td>-0.749193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.253064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.098368</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.149584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128799</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106385</td>\n",
       "      <td>0.028634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.175049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.334342</td>\n",
       "      <td>0.465777</td>\n",
       "      <td>0.183371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.926576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.206685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.199461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.826420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.968406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.190115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.701422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.402246</td>\n",
       "      <td>0.061869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1306 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.059682  0.000000  0.000000 -0.089369  0.000000 -0.296950  0.000000   \n",
       "1     0.000000  0.000000  0.000000 -0.311346 -0.074599  0.000000  0.000000   \n",
       "2     0.000000  0.000000  0.000000  0.200254  0.000000  0.000000  0.000000   \n",
       "3     0.000000 -0.030068  0.000000 -0.047704  0.000000 -0.178866  0.000000   \n",
       "4     0.000000  0.000000 -0.149584  0.000000  0.128799  0.000000  0.014291   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1301  0.000000  0.000000  0.000000 -0.175049  0.000000  0.000000  0.000000   \n",
       "1302  0.000000  0.000000  0.065435  0.000000  0.000000  0.000000  0.000000   \n",
       "1303  0.000000  0.000000 -0.206685  0.000000  0.000000  0.000000 -0.199461   \n",
       "1304  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1305  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "            7         8    9   ...        15        16        17        18  \\\n",
       "0     0.273281 -0.095848  0.0  ...  0.000000 -0.272807  0.000000  0.000000   \n",
       "1     0.000000  0.449604  0.0  ... -0.146963  0.000000  0.182592 -0.166558   \n",
       "2     0.000000 -0.226631  0.0  ... -0.144254  0.227084  0.000000 -0.088230   \n",
       "3     0.000000  0.000000  0.0  ... -0.209635 -0.749193  0.000000  0.000000   \n",
       "4     0.000000  0.000000  0.0  ...  0.000000  0.106385  0.028634  0.000000   \n",
       "...        ...       ...  ...  ...       ...       ...       ...       ...   \n",
       "1301  0.000000  0.000000  0.0  ... -0.334342  0.465777  0.183371  0.000000   \n",
       "1302  0.112363  0.000000  0.0  ...  0.000000  0.926576  0.000000  0.000000   \n",
       "1303  0.000000 -0.826420  0.0  ...  0.000000 -0.968406  0.000000  0.000000   \n",
       "1304  0.000000  0.000000  0.0  ...  0.000000 -0.701422  0.000000  0.000000   \n",
       "1305  0.000000  0.000000  0.0  ...  0.000000  0.402246  0.061869  0.000000   \n",
       "\n",
       "            19        20        21        22        23       24  \n",
       "0     0.000000  0.126148 -0.048960  0.000000  0.000000 -0.09838  \n",
       "1     0.000000  0.000000  0.000000  0.140820  0.000000  0.00000  \n",
       "2     0.062891  0.252543 -0.145355  0.211223  0.000000  0.00000  \n",
       "3     0.000000  0.000000 -0.253064  0.000000 -0.098368  0.00000  \n",
       "4     0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  \n",
       "...        ...       ...       ...       ...       ...      ...  \n",
       "1301  0.000000  0.000000  0.000000  0.118584  0.000000  0.00000  \n",
       "1302  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  \n",
       "1303  0.000000  0.000000 -0.190115  0.000000  0.000000  0.00000  \n",
       "1304  0.000000  0.000000  0.000000  0.000000  0.000000  0.00000  \n",
       "1305  0.000000  0.101439  0.000000  0.192803  0.000000  0.00000  \n",
       "\n",
       "[1306 rows x 25 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b51d02da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.05968182,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        , -0.09837991],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.14081973,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.21122331,\n",
       "          0.        ,  0.        ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        , -0.20668495, ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.19280321,\n",
       "          0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844a2df",
   "metadata": {},
   "source": [
    "## Dataset1- WTG15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "143791ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['T2', 'RH2', 'T1', 'RH1', 'PR1', 'AD1', 'PR2', 'AD2', 'Rain', 'WS1',\n",
       "       'WS3', 'WS4', 'WD1', 'WD3', 'WD4', 'WSHor', 'WDHor', 'WSVer', 'WDVer',\n",
       "       'TI', 'WSH', 'WD_bin', 'tod', 'WVeer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upload the dataset with file_folder, file_name\n",
    "# data_up= uploading_csv('\\Dataset1-Normal_Site','\\data_comp14.csv')\n",
    "X_train= uploading_csv('\\Dataset1-Normal_Site','\\X_train15.csv')\n",
    "X_test= uploading_csv('\\Dataset1-Normal_Site','\\X_test15.csv')\n",
    "y_train= uploading_csv('\\Dataset1-Normal_Site','\\y_train15.csv')\n",
    "y_test= uploading_csv('\\Dataset1-Normal_Site','\\y_test15.csv')\n",
    "\n",
    "X_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e5ee809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Target'], dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c5bf8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC= uploading_csv('\\Dataset1-Normal_Site','\\PC_1.15kgm-3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca7797",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "acf8d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs={\n",
    "    'n_hidden':[0, 1, 2, 3],\n",
    "    'n_neurons': [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'learning_rate': [0.0001, 0.0003, 0.001, 0.003, 0.005, 0.01, 0.03],\n",
    "    'activation':['relu', 'elu', 'selu'],\n",
    "    'optimizer':['Adam', 'Nesterov', 'Momentum', 'Nadam'],\n",
    "    'regularization':[None, 'Dropout', 'Early Stopping'],\n",
    "    'Leaky':[True, False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4b969a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more of the test scores are non-finite: [-0.37749771         nan -2.91082818 -0.77679342 -0.38081569 -0.37757527\n",
      " -0.49022656 -7.67139975 -2.93948711 -3.72904217 -0.35976648 -0.39512298\n",
      " -0.41844858 -0.3662878  -2.82715352 -0.37654306 -0.42046572 -8.01278019\n",
      " -0.37031429 -0.39685519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters :\n",
      "{'regularization': 'Dropout', 'optimizer': 'Nesterov', 'n_neurons': 40, 'n_hidden': 3, 'learning_rate': 0.001, 'input_shape': 24, 'activation': 'relu', 'Leaky': False}\n",
      "\n",
      "--- 10.393785885969798 minutes ---\n",
      "\n",
      "power curve computation performed\n",
      "power curve computation performed\n",
      "Modelling errors for training set:\n",
      "Wind RMSE:  0.537 m/s as root mean\n",
      "Wind MAE:  0.416 m/s in avg\n",
      "Wind MAPE:  5.029 %\n",
      "Power RMSE:  250.097 kW as root mean\n",
      "Power MAE:  163.428 kW in avg\n",
      "Power MAPE:  11.933 %\n",
      "\n",
      "Modelling errors for test set:\n",
      "Wind RMSE:  0.558 m/s as root mean\n",
      "Wind MAE:  0.434 m/s in avg\n",
      "Wind MAPE:  5.303 %\n",
      "Power RMSE:  263.441 kW as root mean\n",
      "Power MAE:  174.427 kW in avg\n",
      "Power MAPE:  12.945 %\n",
      "\n",
      "Showing the results of the modelling: \n",
      "RandomSearch_ NN performed\n"
     ]
    }
   ],
   "source": [
    "model= RandomSearch_NN(X_train, X_test, y_train, y_test, PC, param_distribs, plot_error=False)\n",
    "#iter=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99f2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ff9a321",
   "metadata": {},
   "source": [
    "### Manual modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8676b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={\n",
    "    'n_hidden':3,\n",
    "    'n_neurons': 80,\n",
    "    'learning_rate':0.001,\n",
    "    'activation':'relu',\n",
    "    'optimizer':'Adam',\n",
    "    'regularization':None,\n",
    "    'Leaky':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e58a838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 13.1988 - val_loss: 0.6998\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.5419 - val_loss: 0.4125\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.4115 - val_loss: 0.4375\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3914 - val_loss: 0.3565\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3738 - val_loss: 0.3514\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3665 - val_loss: 0.3360\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3492 - val_loss: 0.3273\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3543 - val_loss: 0.3284\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3498 - val_loss: 0.3198\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3426 - val_loss: 0.3219\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3284 - val_loss: 0.3067\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3245 - val_loss: 0.3232\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3223 - val_loss: 0.2984\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3155 - val_loss: 0.3107\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3150 - val_loss: 0.2942\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3254 - val_loss: 0.2918\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3181 - val_loss: 0.2919\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3063 - val_loss: 0.3054\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3033 - val_loss: 0.3347\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3207 - val_loss: 0.2879\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2946 - val_loss: 0.2872\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2980 - val_loss: 0.2879\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3230 - val_loss: 0.2830\n",
      "Epoch 24/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2962 - val_loss: 0.3054\n",
      "Epoch 25/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2892 - val_loss: 0.3296\n",
      "Epoch 26/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3018 - val_loss: 0.2823\n",
      "Epoch 27/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2804 - val_loss: 0.2840\n",
      "Epoch 28/100\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.2851 - val_loss: 0.2856\n",
      "Epoch 29/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2817 - val_loss: 0.3348\n",
      "Epoch 30/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2942 - val_loss: 0.2718\n",
      "Epoch 31/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2796 - val_loss: 0.2835\n",
      "Epoch 32/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2800 - val_loss: 0.2953\n",
      "Epoch 33/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2854 - val_loss: 0.3173\n",
      "Epoch 34/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2732 - val_loss: 0.2732\n",
      "Epoch 35/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2859 - val_loss: 0.2980\n",
      "Epoch 36/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2862 - val_loss: 0.3256\n",
      "Epoch 37/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2725 - val_loss: 0.3858\n",
      "Epoch 38/100\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.2856 - val_loss: 0.2766\n",
      "Epoch 39/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2983 - val_loss: 0.3333\n",
      "Epoch 40/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2749 - val_loss: 0.3440\n",
      "Epoch 41/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2904 - val_loss: 0.2878\n",
      "Epoch 42/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2703 - val_loss: 0.2670\n",
      "Epoch 43/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2669 - val_loss: 0.2782\n",
      "Epoch 44/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2827 - val_loss: 0.3364\n",
      "Epoch 45/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2743 - val_loss: 0.2687\n",
      "Epoch 46/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.283 - 0s 3ms/step - loss: 0.2866 - val_loss: 0.3992\n",
      "Epoch 47/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2860 - val_loss: 0.2683\n",
      "Epoch 48/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2780 - val_loss: 0.2748\n",
      "Epoch 49/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2658 - val_loss: 0.2594\n",
      "Epoch 50/100\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.2732 - val_loss: 0.2622\n",
      "Epoch 51/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2625 - val_loss: 0.2721\n",
      "Epoch 52/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2654 - val_loss: 0.2745\n",
      "Epoch 53/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2690 - val_loss: 0.2656\n",
      "Epoch 54/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2592 - val_loss: 0.2989\n",
      "Epoch 55/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2620 - val_loss: 0.2647\n",
      "Epoch 56/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2785 - val_loss: 0.2628\n",
      "Epoch 57/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2788 - val_loss: 0.2699\n",
      "Epoch 58/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2540 - val_loss: 0.2664\n",
      "Epoch 59/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2461 - val_loss: 0.2687\n",
      "Epoch 60/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2585 - val_loss: 0.2722\n",
      "Epoch 61/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2637 - val_loss: 0.2739\n",
      "Epoch 62/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2516 - val_loss: 0.2570\n",
      "Epoch 63/100\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.2514 - val_loss: 0.2638\n",
      "Epoch 64/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2662 - val_loss: 0.2687\n",
      "Epoch 65/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2473 - val_loss: 0.2652\n",
      "Epoch 66/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2453 - val_loss: 0.2506\n",
      "Epoch 67/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2544 - val_loss: 0.2738\n",
      "Epoch 68/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2478 - val_loss: 0.2643\n",
      "Epoch 69/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2643 - val_loss: 0.3311\n",
      "Epoch 70/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2538 - val_loss: 0.2676\n",
      "Epoch 71/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2457 - val_loss: 0.2812\n",
      "Epoch 72/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2547 - val_loss: 0.3642\n",
      "Epoch 73/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2583 - val_loss: 0.2849\n",
      "Epoch 74/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2696 - val_loss: 0.2677\n",
      "Epoch 75/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2378 - val_loss: 0.2653\n",
      "Epoch 76/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2385 - val_loss: 0.2919\n",
      "Epoch 77/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2607 - val_loss: 0.2719\n",
      "Epoch 78/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.245 - 0s 2ms/step - loss: 0.2444 - val_loss: 0.2565\n",
      "Epoch 79/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2354 - val_loss: 0.2641\n",
      "Epoch 80/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2479 - val_loss: 0.2646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2408 - val_loss: 0.4155\n",
      "Epoch 82/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2437 - val_loss: 0.2767\n",
      "Epoch 83/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2450 - val_loss: 0.2584\n",
      "Epoch 84/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2347 - val_loss: 0.2684\n",
      "Epoch 85/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2333 - val_loss: 0.2841\n",
      "Epoch 86/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2362 - val_loss: 0.2593\n",
      "Epoch 87/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2413 - val_loss: 0.2665\n",
      "Epoch 88/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2450 - val_loss: 0.2523\n",
      "Epoch 89/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2411 - val_loss: 0.2741\n",
      "Epoch 90/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2352 - val_loss: 0.2611\n",
      "Epoch 91/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2336 - val_loss: 0.2598\n",
      "Epoch 92/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2480 - val_loss: 0.3998\n",
      "Epoch 93/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2326 - val_loss: 0.2774\n",
      "Epoch 94/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2300 - val_loss: 0.3020\n",
      "Epoch 95/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2418 - val_loss: 0.2570\n",
      "Epoch 96/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2256 - val_loss: 0.2676\n",
      "Epoch 97/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2340 - val_loss: 0.2525\n",
      "Epoch 98/100\n",
      "110/110 [==============================] - ETA: 0s - loss: 0.222 - 0s 2ms/step - loss: 0.2222 - val_loss: 0.2596\n",
      "Epoch 99/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2316 - val_loss: 0.2670\n",
      "Epoch 100/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2255 - val_loss: 0.2577\n",
      "power curve computation performed\n",
      "power curve computation performed\n",
      "Modelling errors for training set:\n",
      "Wind RMSE:  0.466 m/s as root mean\n",
      "Wind MAE:  0.355 m/s in avg\n",
      "Wind MAPE:  4.331 %\n",
      "Power RMSE:  216.41 kW as root mean\n",
      "Power MAE:  139.558 kW in avg\n",
      "Power MAPE:  10.371 %\n",
      "\n",
      "Modelling errors for test set:\n",
      "Wind RMSE:  0.524 m/s as root mean\n",
      "Wind MAE:  0.398 m/s in avg\n",
      "Wind MAPE:  4.925 %\n",
      "Power RMSE:  243.483 kW as root mean\n",
      "Power MAE:  157.762 kW in avg\n",
      "Power MAPE:  12.199 %\n",
      "\n",
      "Showing the results of the modelling: \n",
      "NN modelling performed\n"
     ]
    }
   ],
   "source": [
    "model = modelling_NN (X_train, X_test, y_train, y_test, PC, parameters, plot_error=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2c86177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={\n",
    "    'n_hidden':3,\n",
    "    'n_neurons': 80,\n",
    "    'learning_rate':0.001,\n",
    "    'activation':'selu',\n",
    "    'optimizer':'Nesterov',\n",
    "    'regularization':None,\n",
    "    'Leaky':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0b4cb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 3.8616 - val_loss: 0.4515\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4457 - val_loss: 0.4422\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4102 - val_loss: 0.3656\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3905 - val_loss: 0.3917\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3883 - val_loss: 0.3593\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3809 - val_loss: 0.3549\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3745 - val_loss: 0.3497\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3700 - val_loss: 0.4512\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3655 - val_loss: 0.3421\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3661 - val_loss: 0.4181\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3624 - val_loss: 0.3516\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3599 - val_loss: 0.5529\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3597 - val_loss: 0.3706\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3589 - val_loss: 0.3402\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3515 - val_loss: 0.3468\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3554 - val_loss: 0.3364\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3526 - val_loss: 0.3470\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3436 - val_loss: 0.3657\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3481 - val_loss: 0.3508\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3424 - val_loss: 0.3307\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3423 - val_loss: 0.3474\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3402 - val_loss: 0.3270\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3389 - val_loss: 0.3417\n",
      "Epoch 24/100\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.3336 - val_loss: 0.3409\n",
      "Epoch 25/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3378 - val_loss: 0.3255\n",
      "Epoch 26/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3296 - val_loss: 0.3284\n",
      "Epoch 27/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3301 - val_loss: 0.3516\n",
      "Epoch 28/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3361 - val_loss: 0.3242\n",
      "Epoch 29/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3281 - val_loss: 0.3482\n",
      "Epoch 30/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3287 - val_loss: 0.3389\n",
      "Epoch 31/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3266 - val_loss: 0.3143\n",
      "Epoch 32/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3247 - val_loss: 0.3160\n",
      "Epoch 33/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3237 - val_loss: 0.3228\n",
      "Epoch 34/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3228 - val_loss: 0.3164\n",
      "Epoch 35/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3164 - val_loss: 0.3133\n",
      "Epoch 36/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3229 - val_loss: 0.3188\n",
      "Epoch 37/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3229 - val_loss: 0.3174\n",
      "Epoch 38/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3215 - val_loss: 0.3065\n",
      "Epoch 39/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3197 - val_loss: 0.3625\n",
      "Epoch 40/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3150 - val_loss: 0.3249\n",
      "Epoch 41/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3149 - val_loss: 0.3012\n",
      "Epoch 42/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3137 - val_loss: 0.3086\n",
      "Epoch 43/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3163 - val_loss: 0.3054\n",
      "Epoch 44/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3117 - val_loss: 0.3274\n",
      "Epoch 45/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3156 - val_loss: 0.3781\n",
      "Epoch 46/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3137 - val_loss: 0.3017\n",
      "Epoch 47/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3075 - val_loss: 0.3597\n",
      "Epoch 48/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3114 - val_loss: 0.3181\n",
      "Epoch 49/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3075 - val_loss: 0.3349\n",
      "Epoch 50/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3068 - val_loss: 0.3007\n",
      "Epoch 51/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3130 - val_loss: 0.3199\n",
      "Epoch 52/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3096 - val_loss: 0.2994\n",
      "Epoch 53/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3149 - val_loss: 0.2907\n",
      "Epoch 54/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3039 - val_loss: 0.3218\n",
      "Epoch 55/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3034 - val_loss: 0.3593\n",
      "Epoch 56/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3011 - val_loss: 0.2898\n",
      "Epoch 57/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3024 - val_loss: 0.2921\n",
      "Epoch 58/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3020 - val_loss: 0.3177\n",
      "Epoch 59/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3026 - val_loss: 0.3414\n",
      "Epoch 60/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3001 - val_loss: 0.3032\n",
      "Epoch 61/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2935 - val_loss: 0.2845\n",
      "Epoch 62/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2995 - val_loss: 0.2913\n",
      "Epoch 63/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3049 - val_loss: 0.2828\n",
      "Epoch 64/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2966 - val_loss: 0.2958\n",
      "Epoch 65/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2951 - val_loss: 0.2905\n",
      "Epoch 66/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2969 - val_loss: 0.2831\n",
      "Epoch 67/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2928 - val_loss: 0.3622\n",
      "Epoch 68/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2958 - val_loss: 0.2892\n",
      "Epoch 69/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2959 - val_loss: 0.2809\n",
      "Epoch 70/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2925 - val_loss: 0.2880\n",
      "Epoch 71/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2924 - val_loss: 0.2901\n",
      "Epoch 72/100\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.2871 - val_loss: 0.2775\n",
      "Epoch 73/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2900 - val_loss: 0.2794\n",
      "Epoch 74/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2903 - val_loss: 0.2845\n",
      "Epoch 75/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2968 - val_loss: 0.2836\n",
      "Epoch 76/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2876 - val_loss: 0.2830\n",
      "Epoch 77/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2950 - val_loss: 0.2891\n",
      "Epoch 78/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2919 - val_loss: 0.2770\n",
      "Epoch 79/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2930 - val_loss: 0.2727\n",
      "Epoch 80/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2944 - val_loss: 0.2988\n",
      "Epoch 81/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2879 - val_loss: 0.2941\n",
      "Epoch 82/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2863 - val_loss: 0.2894\n",
      "Epoch 83/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2902 - val_loss: 0.2893\n",
      "Epoch 84/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2906 - val_loss: 0.3068\n",
      "Epoch 85/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2913 - val_loss: 0.2715\n",
      "Epoch 86/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2850 - val_loss: 0.2806\n",
      "Epoch 87/100\n",
      "110/110 [==============================] - 1s 6ms/step - loss: 0.2857 - val_loss: 0.2981\n",
      "Epoch 88/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2831 - val_loss: 0.2902\n",
      "Epoch 89/100\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.2838 - val_loss: 0.2743\n",
      "Epoch 90/100\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.2816 - val_loss: 0.3459\n",
      "Epoch 91/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2869 - val_loss: 0.2990\n",
      "Epoch 92/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2852 - val_loss: 0.2795\n",
      "Epoch 93/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2806 - val_loss: 0.2690\n",
      "Epoch 94/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2852 - val_loss: 0.2769\n",
      "Epoch 95/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2810 - val_loss: 0.3234\n",
      "Epoch 96/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2849 - val_loss: 0.2757\n",
      "Epoch 97/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2826 - val_loss: 0.2751\n",
      "Epoch 98/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2782 - val_loss: 0.2768\n",
      "Epoch 99/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2808 - val_loss: 0.2794\n",
      "Epoch 100/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2808 - val_loss: 0.2837\n",
      "power curve computation performed\n",
      "power curve computation performed\n",
      "Modelling errors for training set:\n",
      "Wind RMSE:  0.532 m/s as root mean\n",
      "Wind MAE:  0.409 m/s in avg\n",
      "Wind MAPE:  4.914 %\n",
      "Power RMSE:  243.015 kW as root mean\n",
      "Power MAE:  158.844 kW in avg\n",
      "Power MAPE:  11.58 %\n",
      "\n",
      "Modelling errors for test set:\n",
      "Wind RMSE:  0.547 m/s as root mean\n",
      "Wind MAE:  0.419 m/s in avg\n",
      "Wind MAPE:  5.084 %\n",
      "Power RMSE:  254.944 kW as root mean\n",
      "Power MAE:  166.586 kW in avg\n",
      "Power MAPE:  12.266 %\n",
      "\n",
      "Showing the results of the modelling: \n",
      "NN modelling performed\n"
     ]
    }
   ],
   "source": [
    "model = modelling_NN (X_train, X_test, y_train, y_test, PC, parameters, plot_error=False, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ab8aa31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={\n",
    "    'n_hidden':3,\n",
    "    'n_neurons': 80,\n",
    "    'learning_rate':0.001,\n",
    "    'activation':'elu',\n",
    "    'optimizer':'Adam',\n",
    "    'regularization':None,\n",
    "    'Leaky':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82d1ae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 7.9682 - val_loss: 0.4961\n",
      "Epoch 2/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4697 - val_loss: 0.4083\n",
      "Epoch 3/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.4160 - val_loss: 0.3744\n",
      "Epoch 4/100\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.3865 - val_loss: 0.3681\n",
      "Epoch 5/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3740 - val_loss: 0.3525\n",
      "Epoch 6/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3648 - val_loss: 0.3524\n",
      "Epoch 7/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3562 - val_loss: 0.3667\n",
      "Epoch 8/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3658 - val_loss: 0.3199\n",
      "Epoch 9/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3539 - val_loss: 0.3255\n",
      "Epoch 10/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3378 - val_loss: 0.3303\n",
      "Epoch 11/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3289 - val_loss: 0.3040\n",
      "Epoch 12/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3251 - val_loss: 0.3170\n",
      "Epoch 13/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3205 - val_loss: 0.3013\n",
      "Epoch 14/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3281 - val_loss: 0.3335\n",
      "Epoch 15/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3199 - val_loss: 0.3101\n",
      "Epoch 16/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3118 - val_loss: 0.3242\n",
      "Epoch 17/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3552 - val_loss: 0.3049\n",
      "Epoch 18/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3156 - val_loss: 0.2993\n",
      "Epoch 19/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3323 - val_loss: 0.2805\n",
      "Epoch 20/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.3000 - val_loss: 0.2815\n",
      "Epoch 21/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3063 - val_loss: 0.2799\n",
      "Epoch 22/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2938 - val_loss: 0.3121\n",
      "Epoch 23/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2923 - val_loss: 0.2767\n",
      "Epoch 24/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2842 - val_loss: 0.2856\n",
      "Epoch 25/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2886 - val_loss: 0.2773\n",
      "Epoch 26/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2802 - val_loss: 0.2845\n",
      "Epoch 27/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2945 - val_loss: 0.2649\n",
      "Epoch 28/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2851 - val_loss: 0.2668\n",
      "Epoch 29/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2895 - val_loss: 0.2725\n",
      "Epoch 30/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2738 - val_loss: 0.2743\n",
      "Epoch 31/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2934 - val_loss: 0.2770\n",
      "Epoch 32/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2749 - val_loss: 0.2649\n",
      "Epoch 33/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.3097 - val_loss: 0.2611\n",
      "Epoch 34/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2755 - val_loss: 0.2549\n",
      "Epoch 35/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2856 - val_loss: 0.3059\n",
      "Epoch 36/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2720 - val_loss: 0.3328\n",
      "Epoch 37/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2697 - val_loss: 0.3304\n",
      "Epoch 38/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 0.2478\n",
      "Epoch 39/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2863 - val_loss: 0.2540\n",
      "Epoch 40/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2580 - val_loss: 0.2619\n",
      "Epoch 41/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2702 - val_loss: 0.2811\n",
      "Epoch 42/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2644 - val_loss: 0.2666\n",
      "Epoch 43/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2735 - val_loss: 0.2635\n",
      "Epoch 44/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2591 - val_loss: 0.2470\n",
      "Epoch 45/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2615 - val_loss: 0.2738\n",
      "Epoch 46/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2877 - val_loss: 0.2572\n",
      "Epoch 47/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2682 - val_loss: 0.2634\n",
      "Epoch 48/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2733 - val_loss: 0.2536\n",
      "Epoch 49/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2619 - val_loss: 0.2513\n",
      "Epoch 50/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2519 - val_loss: 0.3083\n",
      "Epoch 51/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2637 - val_loss: 0.2571\n",
      "Epoch 52/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2685 - val_loss: 0.2452\n",
      "Epoch 53/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2545 - val_loss: 0.2428\n",
      "Epoch 54/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2633 - val_loss: 0.2438\n",
      "Epoch 55/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2508 - val_loss: 0.2891\n",
      "Epoch 56/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2468 - val_loss: 0.2659\n",
      "Epoch 57/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2441 - val_loss: 0.2464\n",
      "Epoch 58/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2401 - val_loss: 0.2388\n",
      "Epoch 59/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2527 - val_loss: 0.2609\n",
      "Epoch 60/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2522 - val_loss: 0.2464\n",
      "Epoch 61/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2449 - val_loss: 0.2606\n",
      "Epoch 62/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2424 - val_loss: 0.2397\n",
      "Epoch 63/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2381 - val_loss: 0.2428\n",
      "Epoch 64/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2399 - val_loss: 0.2658\n",
      "Epoch 65/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2433 - val_loss: 0.2552\n",
      "Epoch 66/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2361 - val_loss: 0.2596\n",
      "Epoch 67/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2606 - val_loss: 0.2489\n",
      "Epoch 68/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2382 - val_loss: 0.2365\n",
      "Epoch 69/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2435 - val_loss: 0.2400\n",
      "Epoch 70/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2331 - val_loss: 0.2515\n",
      "Epoch 71/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2354 - val_loss: 0.2454\n",
      "Epoch 72/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2390 - val_loss: 0.2477\n",
      "Epoch 73/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2328 - val_loss: 0.2429\n",
      "Epoch 74/100\n",
      "110/110 [==============================] - 0s 4ms/step - loss: 0.2596 - val_loss: 0.2637\n",
      "Epoch 75/100\n",
      "110/110 [==============================] - 0s 3ms/step - loss: 0.2304 - val_loss: 0.2388\n",
      "Epoch 76/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2347 - val_loss: 0.2472\n",
      "Epoch 77/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2446 - val_loss: 0.2545\n",
      "Epoch 78/100\n",
      "110/110 [==============================] - 0s 2ms/step - loss: 0.2393 - val_loss: 0.2805\n",
      "power curve computation performed\n",
      "power curve computation performed\n",
      "Modelling errors for training set:\n",
      "Wind RMSE:  0.468 m/s as root mean\n",
      "Wind MAE:  0.357 m/s in avg\n",
      "Wind MAPE:  4.361 %\n",
      "Power RMSE:  217.615 kW as root mean\n",
      "Power MAE:  139.753 kW in avg\n",
      "Power MAPE:  10.592 %\n",
      "\n",
      "Modelling errors for test set:\n",
      "Wind RMSE:  0.509 m/s as root mean\n",
      "Wind MAE:  0.388 m/s in avg\n",
      "Wind MAPE:  4.791 %\n",
      "Power RMSE:  238.634 kW as root mean\n",
      "Power MAE:  154.189 kW in avg\n",
      "Power MAPE:  11.971 %\n",
      "\n",
      "Showing the results of the modelling: \n",
      "NN modelling performed\n"
     ]
    }
   ],
   "source": [
    "model = modelling_NN_ES (X_train, X_test, y_train, y_test, PC, parameters, plot_error=False, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8e418",
   "metadata": {},
   "source": [
    "### Model saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0e68dd",
   "metadata": {},
   "source": [
    "parameters={\n",
    "    'n_hidden':3,\n",
    "    'n_neurons': 80,\n",
    "    'learning_rate':0.001,\n",
    "    'activation':'relu',\n",
    "    'optimizer':'Adam',\n",
    "    'regularization':None,\n",
    "    'Leaky':True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c679f5f",
   "metadata": {},
   "source": [
    "MAPE power: 12.199%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce6c5316",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('WTG15_ANN1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786ba4c",
   "metadata": {},
   "source": [
    "parameters={\n",
    "    'n_hidden':3,\n",
    "    'n_neurons': 80,\n",
    "    'learning_rate':0.001,\n",
    "    'activation':'elu',\n",
    "    'optimizer':'Adam',\n",
    "    'regularization':'Early Stopping,\n",
    "    'Leaky':True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1398b58",
   "metadata": {},
   "source": [
    "MAPE power: 11.971%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5132e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('WTG15_ANN2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e998f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11ad48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc041588",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "369fd667",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model('WTG15_ANN2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a049d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n",
      "power curve computation performed\n",
      "power curve computation performed\n",
      "Modelling errors for training set:\n",
      "Wind RMSE:  0.468 m/s as root mean\n",
      "Wind MAE:  0.357 m/s in avg\n",
      "Wind MAPE:  4.361 %\n",
      "Power RMSE:  217.615 kW as root mean\n",
      "Power MAE:  139.753 kW in avg\n",
      "Power MAPE:  10.592 %\n",
      "\n",
      "Modelling errors for test set:\n",
      "Wind RMSE:  0.509 m/s as root mean\n",
      "Wind MAE:  0.388 m/s in avg\n",
      "Wind MAPE:  4.791 %\n",
      "Power RMSE:  238.634 kW as root mean\n",
      "Power MAE:  154.189 kW in avg\n",
      "Power MAPE:  11.971 %\n",
      "\n",
      "Showing the results of the modelling: \n",
      "NN results performed\n"
     ]
    }
   ],
   "source": [
    "WS_pred=model_testing (X_train, X_test, y_train, y_test, PC, model, plot_error=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad86b646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file ANN_WTG15.csv saved in \\Results_ folder\n"
     ]
    }
   ],
   "source": [
    "WS_pred=pd.DataFrame(WS_pred)\n",
    "save(WS_pred,'\\Results_','ANN_WTG15.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577106b1",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ccaadba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6cb55c41b94b5884d996ab87d543b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x684 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance through SHAP values performed\n"
     ]
    }
   ],
   "source": [
    "shap_values=feature_importance (X_train, X_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6a2bc171",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=transform_shap (shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "11181317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variables</th>\n",
       "      <th>SHAP_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T2</td>\n",
       "      <td>0.036240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RH2</td>\n",
       "      <td>0.059350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1</td>\n",
       "      <td>0.140195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RH1</td>\n",
       "      <td>0.013505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PR1</td>\n",
       "      <td>0.100791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AD1</td>\n",
       "      <td>0.076489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PR2</td>\n",
       "      <td>0.077937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AD2</td>\n",
       "      <td>0.008116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rain</td>\n",
       "      <td>0.005398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WS1</td>\n",
       "      <td>0.718561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WS3</td>\n",
       "      <td>0.022044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WS4</td>\n",
       "      <td>0.608895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WD1</td>\n",
       "      <td>0.034025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WD3</td>\n",
       "      <td>0.054790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>WD4</td>\n",
       "      <td>0.015546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WSHor</td>\n",
       "      <td>0.679150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WDHor</td>\n",
       "      <td>0.021230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WSVer</td>\n",
       "      <td>0.020317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>WDVer</td>\n",
       "      <td>0.054425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TI</td>\n",
       "      <td>0.099621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>WSH</td>\n",
       "      <td>0.162675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>WD_bin</td>\n",
       "      <td>0.049535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tod</td>\n",
       "      <td>0.010875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>WVeer</td>\n",
       "      <td>0.012155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variables  SHAP_abs\n",
       "0         T2  0.036240\n",
       "1        RH2  0.059350\n",
       "2         T1  0.140195\n",
       "3        RH1  0.013505\n",
       "4        PR1  0.100791\n",
       "5        AD1  0.076489\n",
       "6        PR2  0.077937\n",
       "7        AD2  0.008116\n",
       "8       Rain  0.005398\n",
       "9        WS1  0.718561\n",
       "10       WS3  0.022044\n",
       "11       WS4  0.608895\n",
       "12       WD1  0.034025\n",
       "13       WD3  0.054790\n",
       "14       WD4  0.015546\n",
       "15     WSHor  0.679150\n",
       "16     WDHor  0.021230\n",
       "17     WSVer  0.020317\n",
       "18     WDVer  0.054425\n",
       "19        TI  0.099621\n",
       "20       WSH  0.162675\n",
       "21    WD_bin  0.049535\n",
       "22       tod  0.010875\n",
       "23     WVeer  0.012155"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754bb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "354.74px",
    "left": "1350.99px",
    "right": "20px",
    "top": "2px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
